{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a0e698b",
   "metadata": {},
   "source": [
    "# Instant Neural Graphics Primitives with a Multiresolution Hash Encoding\n",
    "\n",
    "- Ref: \n",
    "    - https://arxiv.org/abs/2201.05989\n",
    "    - https://github.com/MaximeVandegar/Papers-in-100-Lines-of-Code/tree/main/Instant_Neural_Graphics_Primitives_with_a_Multiresolution_Hash_Encoding\n",
    "\n",
    "- Conda env : [gsplat](../gsplat/README.md#setup-a-conda-environment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9c4eab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchshow as ts\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7160424d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping already downloaded file ./temp/data/training_data.pkl\n",
      "Skipping already downloaded file ./temp/data/testing_data.pkl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./temp/data/testing_data.pkl'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download Data\n",
    "Path('./temp/data').mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "url = 'https://drive.google.com/uc?id=1hH7NhaXxIthO9-FeT16fvpf_MVIhf41J'\n",
    "train_data_path = './temp/data/training_data.pkl'\n",
    "gdown.download(url, train_data_path, quiet=False, resume=True)\n",
    "\n",
    "\n",
    "url = 'https://drive.google.com/uc?id=16M64h0KKgFKhM8hJDpqd15YWYhafUs2Q'\n",
    "test_data_path = './temp/data/testing_data.pkl'\n",
    "gdown.download(url, test_data_path, quiet=False, resume=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12c95d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_model_path = './temp/07_InstantNeRF_models/'\n",
    "g_video_path = './temp/07_InstantNeRF_novel_views/'\n",
    "fps = 10\n",
    "Path(g_model_path).mkdir(exist_ok=True, parents=True)\n",
    "Path(g_video_path).mkdir(exist_ok=True, parents=True)\n",
    "training_dataset = torch.from_numpy(np.load('./temp/data/training_data.pkl', allow_pickle=True))\n",
    "testing_dataset = torch.from_numpy(np.load('./temp/data/testing_data.pkl', allow_pickle=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0552a80c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current memory allocated on MPS: 0 bytes\n",
      "Driver memory allocated on MPS: 393216 bytes\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    g_device = torch.device(\"mps\")\n",
    "    print(f\"Current memory allocated on MPS: {torch.mps.current_allocated_memory()} bytes\")\n",
    "    print(f\"Driver memory allocated on MPS: {torch.mps.driver_allocated_memory()} bytes\")\n",
    "elif torch.cuda.is_available():\n",
    "    g_device = torch.device(\"cuda\")\n",
    "else:\n",
    "    g_device = torch.device(\"cpu\")\n",
    "g_device = torch.device(\"cpu\")\n",
    "print(g_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8b6eefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "\n",
    "class NGP(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, T, Nl, L, device, aabb_scale, F=2):\n",
    "        super(NGP, self).__init__()\n",
    "        self.T = T\n",
    "        self.Nl = Nl\n",
    "        self.F = F\n",
    "        self.L = L  # For encoding directions\n",
    "        self.aabb_scale = aabb_scale\n",
    "        self.lookup_tables = torch.nn.ParameterDict(\n",
    "            {str(i): torch.nn.Parameter((torch.rand(\n",
    "                (T, 2), device=device) * 2 - 1) * 1e-4) for i in range(len(Nl))})\n",
    "        self.pi1, self.pi2, self.pi3 = 1, 2_654_435_761, 805_459_861\n",
    "        self.density_MLP = nn.Sequential(nn.Linear(self.F * len(Nl), 64),\n",
    "                                         nn.ReLU(), nn.Linear(64, 16)).to(device)\n",
    "        self.color_MLP = nn.Sequential(nn.Linear(27 + 16, 64), nn.ReLU(),\n",
    "                                       nn.Linear(64, 64), nn.ReLU(),\n",
    "                                       nn.Linear(64, 3), nn.Sigmoid()).to(device)\n",
    "\n",
    "    def positional_encoding(self, x):\n",
    "        out = [x]\n",
    "        for j in range(self.L):\n",
    "            out.append(torch.sin(2 ** j * x))\n",
    "            out.append(torch.cos(2 ** j * x))\n",
    "        return torch.cat(out, dim=1)\n",
    "\n",
    "    def forward(self, x, d):\n",
    "\n",
    "        x /= self.aabb_scale\n",
    "        mask = (x[:, 0].abs() < .5) & (x[:, 1].abs() < .5) & (x[:, 2].abs() < .5)\n",
    "        x += 0.5  # x in [0, 1]^3\n",
    "\n",
    "        color = torch.zeros((x.shape[0], 3), device=x.device)\n",
    "        log_sigma = torch.zeros((x.shape[0]), device=x.device) - 100000\n",
    "        features = torch.empty((x[mask].shape[0], self.F * len(self.Nl)), device=x.device)\n",
    "        for i, N in enumerate(self.Nl):\n",
    "            # Computing vertices, use nn.functional.grid_sample convention\n",
    "            floor = torch.floor(x[mask] * N)\n",
    "            ceil = torch.ceil(x[mask] * N)\n",
    "            vertices = torch.zeros((x[mask].shape[0], 8, 3), dtype=torch.int64, device=x.device)\n",
    "            vertices[:, 0] = floor\n",
    "            vertices[:, 1] = torch.cat((ceil[:, 0, None], floor[:, 1, None], floor[:, 2, None]), dim=1)\n",
    "            vertices[:, 2] = torch.cat((floor[:, 0, None], ceil[:, 1, None], floor[:, 2, None]), dim=1)\n",
    "            vertices[:, 4] = torch.cat((floor[:, 0, None], floor[:, 1, None], ceil[:, 2, None]), dim=1)\n",
    "            vertices[:, 6] = torch.cat((floor[:, 0, None], ceil[:, 1, None], ceil[:, 2, None]), dim=1)\n",
    "            vertices[:, 5] = torch.cat((ceil[:, 0, None], floor[:, 1, None], ceil[:, 2, None]), dim=1)\n",
    "            vertices[:, 3] = torch.cat((ceil[:, 0, None], ceil[:, 1, None], floor[:, 2, None]), dim=1)\n",
    "            vertices[:, 7] = ceil\n",
    "\n",
    "            # hashing\n",
    "            a = vertices[:, :, 0] * self.pi1\n",
    "            b = vertices[:, :, 1] * self.pi2\n",
    "            c = vertices[:, :, 2] * self.pi3\n",
    "            h_x = torch.remainder(torch.bitwise_xor(torch.bitwise_xor(a, b), c), self.T)\n",
    "\n",
    "            # Lookup\n",
    "            looked_up = self.lookup_tables[str(i)][h_x].transpose(-1, -2)\n",
    "            volume = looked_up.reshape((looked_up.shape[0], 2, 2, 2, 2))\n",
    "            features[:, i*2:(i+1)*2] = torch.nn.functional.grid_sample(\n",
    "                volume,\n",
    "                ((x[mask] * N - floor) - 0.5).unsqueeze(1).unsqueeze(1).unsqueeze(1)\n",
    "                ).squeeze(-1).squeeze(-1).squeeze(-1)\n",
    "\n",
    "        xi = self.positional_encoding(d[mask])\n",
    "        h = self.density_MLP(features)\n",
    "        log_sigma[mask] = h[:, 0]\n",
    "        color[mask] = self.color_MLP(torch.cat((h, xi), dim=1))\n",
    "        return color, torch.exp(log_sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fe5712b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rendering\n",
    "\n",
    "def compute_accumulated_transmittance(alphas):\n",
    "    accumulated_transmittance = torch.cumprod(alphas, 1)\n",
    "    return torch.cat((torch.ones((accumulated_transmittance.shape[0], 1), device=alphas.device),\n",
    "                      accumulated_transmittance[:, :-1]), dim=-1)\n",
    "\n",
    "\n",
    "def render_rays(nerf_model, ray_origins, ray_directions, hn=0, hf=0.5, nb_bins=192):\n",
    "    device = ray_origins.device\n",
    "    t = torch.linspace(hn, hf, nb_bins, device=device).expand(ray_origins.shape[0], nb_bins)\n",
    "    # Perturb sampling along each ray.\n",
    "    mid = (t[:, :-1] + t[:, 1:]) / 2.\n",
    "    lower = torch.cat((t[:, :1], mid), -1)\n",
    "    upper = torch.cat((mid, t[:, -1:]), -1)\n",
    "    u = torch.rand(t.shape, device=device)\n",
    "    t = lower + (upper - lower) * u  # [batch_size, nb_bins]\n",
    "    delta = torch.cat((t[:, 1:] - t[:, :-1], torch.tensor(\n",
    "        [1e10], device=device).expand(ray_origins.shape[0], 1)), -1)\n",
    "\n",
    "    # Compute the 3D points along each ray\n",
    "    x = ray_origins.unsqueeze(1) + t.unsqueeze(2) * ray_directions.unsqueeze(1)\n",
    "    # Expand the ray_directions tensor to match the shape of x\n",
    "    ray_directions = ray_directions.expand(nb_bins, ray_directions.shape[0], 3).transpose(0, 1)\n",
    "    colors, sigma = nerf_model(x.reshape(-1, 3), ray_directions.reshape(-1, 3))\n",
    "    alpha = 1 - torch.exp(-sigma.reshape(x.shape[:-1]) * delta)  # [batch_size, nb_bins]\n",
    "    weights = compute_accumulated_transmittance(1 - alpha).unsqueeze(2) * alpha.unsqueeze(2)\n",
    "    c = (weights * colors.reshape(x.shape)).sum(dim=1)\n",
    "    weight_sum = weights.sum(-1).sum(-1)  # Regularization for white background\n",
    "    return c + 1 - weight_sum.unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba60b400",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test(nerf_model, hn, hf, dataset, chunk_size=5, img_index=0, nb_bins=192, H=400, W=400):\n",
    "    ray_origins = dataset[img_index * H * W: (img_index + 1) * H * W, :3]\n",
    "    ray_directions = dataset[img_index * H * W: (img_index + 1) * H * W, 3:6]\n",
    "\n",
    "    data = []\n",
    "    for i in range(int(np.ceil(H / chunk_size))):\n",
    "        ray_origins_ = ray_origins[i * W * chunk_size: (i + 1) * W * chunk_size].to(g_device)\n",
    "        ray_directions_ = ray_directions[i * W * chunk_size: (i + 1) * W * chunk_size].to(g_device)\n",
    "        regenerated_px_values = render_rays(nerf_model, ray_origins_, ray_directions_, hn=hn, hf=hf, nb_bins=nb_bins)\n",
    "        data.append(regenerated_px_values.cpu())\n",
    "    img_t = torch.cat(data).reshape(H, W, 3)\n",
    "    return img_t\n",
    "\n",
    "def train(nerf_model, optimizer, data_loader, device='cpu', hn=0, hf=1, start_epoch = 0, nb_epochs=int(1e5), nb_bins=192, H=400, W=400, eval_steps = 5):\n",
    "    training_loss = []\n",
    "    for e in (range(start_epoch, nb_epochs)):\n",
    "        print(f\"{e:03d}-epoch\")\n",
    "        for ep, batch in enumerate(tqdm(data_loader, total = len(data_loader), desc=\"Training\")):\n",
    "            ray_origins = batch[:, :3].to(device)\n",
    "            ray_directions = batch[:, 3:6].to(device)\n",
    "            ground_truth_px_values = batch[:, 6:].to(device)\n",
    "\n",
    "            regenerated_px_values = render_rays(nerf_model, ray_origins, ray_directions, hn=hn, hf=hf, nb_bins=nb_bins)\n",
    "            loss = ((ground_truth_px_values - regenerated_px_values) ** 2).sum()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            training_loss.append(loss.item())\n",
    "        _model_path = os.path.join(g_model_path, f'nerf_model_{e:03d}.pth')\n",
    "        torch.save(nerf_model.state_dict(), _model_path)\n",
    "        # Export in TorchScript Format\n",
    "        # _model_script_path = os.path.join(g_model_path, f'nerf_model_{e:03d}.pt')\n",
    "        # _model_scripted = torch.jit.script(nerf_model)\n",
    "        # _model_scripted.save(_model_script_path)\n",
    "\n",
    "        if e % eval_steps == 0 or e == nb_epochs - 1:\n",
    "            nerf_model.eval()\n",
    "            imgT_lst = []\n",
    "            for idx in tqdm(range(200), desc=\"Validation\"):\n",
    "                imgT_lst.append(test(nerf_model, 2, 6, testing_dataset, chunk_size = 10, img_index=idx, nb_bins=192, H = H, W = W ).unsqueeze(0))\n",
    "            img_t = (torch.cat(imgT_lst) * 255).to(torch.uint8)\n",
    "            _video_path = os.path.join(g_video_path,  f'nerf_model_{e:03d}.mp4')\n",
    "            torchvision.io.write_video(_video_path, img_t, fps)\n",
    "            nerf_model.train()\n",
    "\n",
    "    return training_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d2b215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000-epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/15625 [00:00<?, ?it/s]/Users/hyunjae.k/110_HyunJae_Git/2025_Drills/.venv/lib/python3.10/site-packages/torch/nn/functional.py:5015: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  warnings.warn(\n",
      "Training: 100%|██████████| 15625/15625 [1:20:44<00:00,  3.23it/s]\n",
      "Validation:  25%|██▌       | 50/200 [1:18:28<3:55:25, 94.17s/it]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[800, 800, 3]' is invalid for input of size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# scheduler = torch.optim.lr_scheduler.MultiStepLR(model_optimizer, milestones=[2, 4, 8], gamma=0.5)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m data_loader \u001b[38;5;241m=\u001b[39m DataLoader(training_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 17\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnb_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mg_device\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnb_bins\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m192\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mH\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m800\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m800\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# for img_index in range(200):\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m#     test(model, 2, 6, testing_dataset, img_index=img_index, nb_bins=192, H=800, W=800)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m imgT_lst \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[0;32mIn[7], line 42\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(nerf_model, optimizer, data_loader, device, hn, hf, start_epoch, nb_epochs, nb_bins, H, W, eval_steps)\u001b[0m\n\u001b[1;32m     40\u001b[0m imgT_lst \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m200\u001b[39m), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 42\u001b[0m     imgT_lst\u001b[38;5;241m.\u001b[39mappend(\u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnerf_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtesting_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnb_bins\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m192\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mH\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m     43\u001b[0m img_t \u001b[38;5;241m=\u001b[39m (torch\u001b[38;5;241m.\u001b[39mcat(imgT_lst) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m255\u001b[39m)\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39muint8)\n\u001b[1;32m     44\u001b[0m _video_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(g_video_path,  \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnerf_model_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m03d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.mp4\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/110_HyunJae_Git/2025_Drills/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 12\u001b[0m, in \u001b[0;36mtest\u001b[0;34m(nerf_model, hn, hf, dataset, chunk_size, img_index, nb_bins, H, W)\u001b[0m\n\u001b[1;32m     10\u001b[0m     regenerated_px_values \u001b[38;5;241m=\u001b[39m render_rays(nerf_model, ray_origins_, ray_directions_, hn\u001b[38;5;241m=\u001b[39mhn, hf\u001b[38;5;241m=\u001b[39mhf, nb_bins\u001b[38;5;241m=\u001b[39mnb_bins)\n\u001b[1;32m     11\u001b[0m     data\u001b[38;5;241m.\u001b[39mappend(regenerated_px_values\u001b[38;5;241m.\u001b[39mcpu())\n\u001b[0;32m---> 12\u001b[0m img_t \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m img_t\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[800, 800, 3]' is invalid for input of size 0"
     ]
    }
   ],
   "source": [
    "\n",
    "L = 16\n",
    "F = 2\n",
    "T = 2**19\n",
    "N_min = 16\n",
    "N_max = 2048\n",
    "b = np.exp((np.log(N_max) - np.log(N_min)) / (L - 1))\n",
    "Nl = [int(np.floor(N_min * b**l)) for l in range(L)]\n",
    "model = NGP(T, Nl, 4, g_device, 3)\n",
    "model.to(g_device)\n",
    "model_optimizer = torch.optim.Adam(\n",
    "        [{\"params\": model.lookup_tables.parameters(), \"lr\": 1e-2, \"betas\": (0.9, 0.99), \"eps\": 1e-15, \"weight_decay\": 0.},\n",
    "         {\"params\": model.density_MLP.parameters(), \"lr\": 1e-2,  \"betas\": (0.9, 0.99), \"eps\": 1e-15, \"weight_decay\": 10**-6},\n",
    "         {\"params\": model.color_MLP.parameters(), \"lr\": 1e-2,  \"betas\": (0.9, 0.99), \"eps\": 1e-15, \"weight_decay\": 10**-6}])\n",
    "# scheduler = torch.optim.lr_scheduler.MultiStepLR(model_optimizer, milestones=[2, 4, 8], gamma=0.5)\n",
    "\n",
    "data_loader = DataLoader(training_dataset, batch_size=1024, shuffle=True)\n",
    "train(model, model_optimizer, data_loader, nb_epochs=1, device=g_device, hn=2, hf=6, nb_bins=192, H=400, W=400)\n",
    "# for img_index in range(200):\n",
    "#     test(model, 2, 6, testing_dataset, img_index=img_index, nb_bins=192, H=800, W=800)\n",
    "imgT_lst = []\n",
    "for idx in tqdm(range(200), desc=\"Validation\"):\n",
    "        imgT_lst.append(test(model, 2, 6, testing_dataset, chunk_size = 10, img_index=idx, nb_bins=192, H = 400, W = 400 ).unsqueeze(0))\n",
    "img_t = (torch.cat(imgT_lst) * 255).to(torch.uint8)\n",
    "_video_path = os.path.join(g_video_path,  f'nerf_model_test.mp4')\n",
    "torchvision.io.write_video(_video_path, img_t, fps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15999b94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
