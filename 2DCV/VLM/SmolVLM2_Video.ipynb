{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77683888",
   "metadata": {},
   "source": [
    "# Generating Video Highlights Using the SmolVLM2 Model\n",
    "\n",
    "<img src=\"https://pyimagesearch.com/wp-content/uploads/2025/06/generating-video-highlights-using-the-smolvlm2-model-featured-v2.png\" alt=\"Your image title\" width=100% height=100%/>\n",
    "\n",
    "----\n",
    "\n",
    "### Conda env : [cv_playgrounds](../README.md#setup-a-conda-environment)\n",
    "\n",
    "----\n",
    "\n",
    "### Reference:\n",
    "\n",
    "- ***Blogs***\n",
    "    - [Generating Video Highlights Using the SmolVLM2 Model](https://pyimagesearch.com/2025/06/30/generating-video-highlights-using-the-smolvlm2-model/)\n",
    "    - [SmolVLM2: Bringing Video Understanding to Every Device](https://huggingface.co/blog/smolvlm2)\n",
    "    - https://huggingface.co/HuggingFaceTB/SmolVLM-256M-Instruct\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b09f7cf",
   "metadata": {},
   "source": [
    "## Configuring Your Development Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc598956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Sep 18 12:05:51 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 575.57.08              Driver Version: 575.57.08      CUDA Version: 12.9     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 2080 Ti     On  |   00000000:01:00.0  On |                  N/A |\n",
      "| 24%   45C    P5             38W /  250W |    1485MiB /  11264MiB |     33%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A            2105      G   /usr/lib/xorg/Xorg                      652MiB |\n",
      "|    0   N/A  N/A            2285      G   /usr/bin/gnome-shell                     90MiB |\n",
      "|    0   N/A  N/A            4554      G   ...ersion=20250917-050038.719000         79MiB |\n",
      "|    0   N/A  N/A            6126      G   /usr/share/code/code                    600MiB |\n",
      "|    0   N/A  N/A           21502      G   ...ess --variations-seed-version         22MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "Available device : cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    g_device = \"mps\"\n",
    "elif torch.cuda.is_available():\n",
    "    g_device = \"cuda\"\n",
    "    !nvidia-smi\n",
    "else:\n",
    "    g_device = \"cpu\"\n",
    "\n",
    "print(f\"Available device : {g_device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042d18d5",
   "metadata": {},
   "source": [
    "### Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f12d4fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import tempfile\n",
    "import gradio as gr\n",
    "import logging\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from transformers import AutoProcessor, AutoModelForImageTextToText"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef537937",
   "metadata": {},
   "source": [
    "### Setup Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08eff142",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d7b70a",
   "metadata": {},
   "source": [
    "### Get Video Duration in Seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be38eac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_duration_seconds(video_path: str) -> float:\n",
    "   \"\"\"Use ffprobe to get video duration in seconds.\"\"\"\n",
    "   cmd = [\n",
    "       \"ffprobe\",\n",
    "       \"-v\", \"quiet\",\n",
    "       \"-print_format\", \"json\",\n",
    "       \"-show_format\",\n",
    "       video_path\n",
    "   ]\n",
    "   result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "   info = json.loads(result.stdout)\n",
    "   return float(info[\"format\"][\"duration\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfee2f1",
   "metadata": {},
   "source": [
    "### Load Model and Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78ebb457",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_processor(model_path: str, device: str = \"cuda\", dtype=torch.bfloat16):\n",
    "   processor = AutoProcessor.from_pretrained(model_path)\n",
    "   model = AutoModelForImageTextToText.from_pretrained(\n",
    "       model_path,\n",
    "       torch_dtype=dtype,\n",
    "    #    _attn_implementation=\"flash_attention_2\"\n",
    "   ).to(device)\n",
    "   return processor, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d1f42a",
   "metadata": {},
   "source": [
    "### Analyze Video Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aefa1e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_video_content(processor, model, video_path: str, device: str = \"cuda\") -> str:\n",
    "   system_message = \"You are a helpful assistant that can understand videos. Describe what type of video this is and what's happening in it.\"\n",
    "   messages = [\n",
    "       {\n",
    "           \"role\": \"system\",\n",
    "           \"content\": [{\"type\": \"text\", \"text\": system_message}]\n",
    "       },\n",
    "       {\n",
    "           \"role\": \"user\",\n",
    "           \"content\": [\n",
    "               {\"type\": \"video\", \"path\": video_path},\n",
    "               {\"type\": \"text\", \"text\": \"What type of video is this and what's happening in it? Be specific about the content type and general activities you observe.\"}\n",
    "           ]\n",
    "       }\n",
    "   ]\n",
    "   inputs = processor.apply_chat_template(\n",
    "       messages,\n",
    "       add_generation_prompt=True,\n",
    "       tokenize=True,\n",
    "       return_dict=True,\n",
    "       return_tensors=\"pt\"\n",
    "   ).to(device, dtype=torch.bfloat16)\n",
    "   outputs = model.generate(**inputs, max_new_tokens=512, do_sample=True, temperature=0.7)\n",
    "   return processor.decode(outputs[0], skip_special_tokens=True).lower().split(\"assistant: \")[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950508d0",
   "metadata": {},
   "source": [
    "### Determine Highlights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e261021",
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_highlights(processor, model, video_description: str, prompt_num: int = 1, device: str = \"cuda\") -> str:\n",
    "   system_prompts = {\n",
    "       1: \"You are a highlight editor. List archetypal dramatic moments that would make compelling highlights if they appear in the video. Each moment should be specific enough to be recognizable but generic enough to potentially exist in other videos of this type.\",\n",
    "       2: \"You are a helpful visual-language assistant that can understand videos and edit. You are tasked with helping the user to create highlight reels for videos. Highlights should be rare and important events in the video in question.\"\n",
    "   }\n",
    "   user_prompts = {\n",
    "       1: \"List potential highlight moments to look for in this video:\",\n",
    "       2: \"List dramatic moments that would make compelling highlights if they appear in the video. Each moment should be specific enough to be recognizable but generic enough to potentially exist in any video of this type:\"\n",
    "   }\n",
    "   messages = [\n",
    "       {\n",
    "           \"role\": \"system\",\n",
    "           \"content\": [{\"type\": \"text\", \"text\": system_prompts[prompt_num]}]\n",
    "       },\n",
    "       {\n",
    "           \"role\": \"user\",\n",
    "           \"content\": [{\"type\": \"text\", \"text\": f\"\"\"Here is a description of a video:\\n\\n{video_description}\\n\\n{user_prompts[prompt_num]}\"\"\"}]\n",
    "       }\n",
    "   ]\n",
    "   print(f\"Using prompt {prompt_num} for highlight detection\")\n",
    "   print(messages)\n",
    "   inputs = processor.apply_chat_template(\n",
    "       messages,\n",
    "       add_generation_prompt=True,\n",
    "       tokenize=True,\n",
    "       return_dict=True,\n",
    "       return_tensors=\"pt\"\n",
    "   ).to(device, dtype=torch.bfloat16)\n",
    "   outputs = model.generate(**inputs, max_new_tokens=256, do_sample=True, temperature=0.7)\n",
    "   return processor.decode(outputs[0], skip_special_tokens=True).split(\"Assistant: \")[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caca6e38",
   "metadata": {},
   "source": [
    "### Process Video Segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8f5644e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_segment(processor, model, video_path: str, highlight_types: str, device: str = \"cuda\") -> bool:\n",
    "   messages = [\n",
    "       {\n",
    "           \"role\": \"system\",\n",
    "           \"content\": [{\"type\": \"text\", \"text\": \"You are a video highlight analyzer. Your role is to identify moments that have high dramatic value, focusing on displays of skill, emotion, personality, or tension. Compare video segments against provided example highlights to find moments with similar emotional impact and visual interest, even if the specific actions differ.\"}]\n",
    "       },\n",
    "       {\n",
    "           \"role\": \"user\",\n",
    "           \"content\": [\n",
    "               {\"type\": \"video\", \"path\": video_path},\n",
    "               {\"type\": \"text\", \"text\": f\"\"\"Given these highlight examples:\\n{highlight_types}\\n\\nDoes this video contain a moment that matches the core action of one of the highlights? Answer with:\\n'yes' or 'no'\\nIf yes, justify it\"\"\"}]\n",
    "       }\n",
    "   ]\n",
    "   print(messages)\n",
    "   inputs = processor.apply_chat_template(\n",
    "       messages,\n",
    "       add_generation_prompt=True,\n",
    "       tokenize=True,\n",
    "       return_dict=True,\n",
    "       return_tensors=\"pt\"\n",
    "   ).to(device, dtype=torch.bfloat16)\n",
    "   outputs = model.generate(**inputs, max_new_tokens=64, do_sample=False)\n",
    "   response = processor.decode(outputs[0], skip_special_tokens=True).lower().split(\"assistant: \")[1]\n",
    "   print(f\"Segment response {response}\")\n",
    "   return \"yes\" in response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f443c1a4",
   "metadata": {},
   "source": [
    "### Concatenating Video Scenes into a Final Highlight Reel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d60be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_scenes(video_path: str, scene_times: list, output_path: str):\n",
    "    \"\"\"Concatenate selected scenes into final video.\"\"\"\n",
    "    if not scene_times:\n",
    "        logger.warning(\"No scenes to concatenate, skipping.\")\n",
    "        return\n",
    "    filter_complex_parts = []\n",
    "    concat_inputs = []\n",
    "    for i, (start_sec, end_sec) in enumerate(scene_times):\n",
    "        filter_complex_parts.append(\n",
    "            f\"[0:v]trim=start={start_sec}:end={end_sec},\"\n",
    "            f\"setpts=PTS-STARTPTS[v{i}];\"\n",
    "        )\n",
    "        filter_complex_parts.append(\n",
    "            f\"[0:a]atrim=start={start_sec}:end={end_sec},\"\n",
    "            f\"asetpts=PTS-STARTPTS[a{i}];\"\n",
    "        )\n",
    "        concat_inputs.append(f\"[v{i}][a{i}]\")\n",
    "    concat_filter = f\"{''.join(concat_inputs)}concat=n={len(scene_times)}:v=1:a=1[outv][outa]\"\n",
    "    filter_complex = \"\".join(filter_complex_parts) + concat_filter\n",
    "    cmd = [\n",
    "         \"ffmpeg\",\n",
    "         \"-y\",\n",
    "         \"-i\", video_path,\n",
    "         \"-filter_complex\", filter_complex,\n",
    "         \"-map\", \"[outv]\",\n",
    "         \"-map\", \"[outa]\",\n",
    "         \"-c:v\", \"libx264\",\n",
    "         \"-c:a\", \"aac\",\n",
    "         output_path\n",
    "     ]\n",
    "    logger.info(f\"Running ffmpeg command: {' '.join(cmd)}\")\n",
    "    subprocess.run(cmd, check=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc910a91",
   "metadata": {},
   "source": [
    "### Interface Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4322843a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ui(model_path: str):\n",
    "    with gr.Blocks() as app:\n",
    "        gr.Markdown(\"## Generate Video Highlights Using SmolVLM2 Model\")\n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=1):\n",
    "                input_video = gr.Video(\n",
    "                    label=\"Upload your video (max 30 minutes)\",\n",
    "                    interactive=True\n",
    "                )\n",
    "                process_btn = gr.Button(\"Process Video\", variant=\"primary\")\n",
    "            with gr.Column(scale=1):\n",
    "                output_video = gr.Video(\n",
    "                    label=\"Highlight Video\",\n",
    "                    visible=False,\n",
    "                    interactive=False,\n",
    "                )\n",
    "                status = gr.Markdown()\n",
    "                analysis_accordion = gr.Accordion(\n",
    "                    \"Chain of thought details\",\n",
    "                    open=True,\n",
    "                    visible=False\n",
    "                )\n",
    "                with analysis_accordion:\n",
    "                    video_description = gr.Markdown(\"\", elem_id=\"video_desc\")\n",
    "                    highlight_types = gr.Markdown(\"\", elem_id=\"highlight_types\")\n",
    "        def on_process(video):\n",
    "            # Clear all components when starting new processing\n",
    "            yield [\n",
    "                \"\",  # Clear status\n",
    "                \"\",  # Clear video description\n",
    "                \"\",  # Clear highlight types\n",
    "                gr.update(value=None, visible=False),  # Clear video\n",
    "                gr.update(visible=False)  # Hide accordion\n",
    "            ]\n",
    "            if not video:\n",
    "                yield [\n",
    "                    \"Please upload a video\",\n",
    "                    \"\",\n",
    "                    \"\",\n",
    "                    gr.update(visible=False),\n",
    "                    gr.update(visible=False)\n",
    "                ]\n",
    "                return\n",
    "            try:\n",
    "                duration = get_video_duration_seconds(video)\n",
    "                if duration > 1800:  # 30 minutes\n",
    "                    yield [\n",
    "                        \"Video must be shorter than 30 minutes\",\n",
    "                        \"\",\n",
    "                        \"\",\n",
    "                        gr.update(visible=False),\n",
    "                        gr.update(visible=False)\n",
    "                    ]\n",
    "                    return\n",
    "                yield [\n",
    "                    \"Initializing video highlight detector...\",\n",
    "                    \"\",\n",
    "                    \"\",\n",
    "                    gr.update(visible=False),\n",
    "                    gr.update(visible=False)\n",
    "                ]\n",
    "                processor, model = load_model_and_processor(model_path)\n",
    "                yield [\n",
    "                    \"Analyzing video content...\",\n",
    "                    \"\",\n",
    "                    \"\",\n",
    "                    gr.update(visible=False),\n",
    "                    gr.update(visible=True)\n",
    "                ]\n",
    "                video_desc = analyze_video_content(processor, model, video)\n",
    "                formatted_desc = f\"### Summary:\\n {video_desc[:500] + '...' if len(video_desc) > 500 else video_desc}\"\n",
    "                yield [\n",
    "                    \"Determining highlight types (2 variations)...\",\n",
    "                    formatted_desc,\n",
    "                    \"\",\n",
    "                    gr.update(visible=False),\n",
    "                    gr.update(visible=True)\n",
    "                ]\n",
    "                # Get two different sets of highlights\n",
    "                highlights1 = determine_highlights(processor, model, video_desc, prompt_num=1)\n",
    "                highlights2 = determine_highlights(processor, model, video_desc, prompt_num=2)\n",
    "                formatted_highlights = f\"### Highlights to search for:\\nSet 1:\\n{highlights1[:500] + '...' if len(highlights1) > 500 else highlights1}\\n\\nSet 2:\\n{highlights2[:500] + '...' if len(highlights2) > 500 else highlights2}\"\n",
    "                # Split video into segments\n",
    "                temp_dir = \"temp_segments\"\n",
    "                os.makedirs(temp_dir, exist_ok=True)\n",
    "                segment_length = 10.0\n",
    "                duration = get_video_duration_seconds(video)\n",
    "                kept_segments1 = []\n",
    "                kept_segments2 = []\n",
    "                segments_processed = 0\n",
    "                total_segments = int(duration / segment_length)\n",
    "                for start_time in range(0, int(duration), int(segment_length)):\n",
    "                    progress = int((segments_processed / total_segments) * 100)\n",
    "                    yield [\n",
    "                        f\"Processing segments... {progress}% complete\",\n",
    "                        formatted_desc,\n",
    "                        formatted_highlights,\n",
    "                        gr.update(visible=False),\n",
    "                        gr.update(visible=True)\n",
    "                    ]\n",
    "                    # Create segment\n",
    "                    segment_path = f\"{temp_dir}/segment_{start_time}.mp4\"\n",
    "                    end_time = min(start_time + segment_length, duration)\n",
    "                    cmd = [\n",
    "                        \"ffmpeg\",\n",
    "                        \"-y\",\n",
    "                        \"-i\", video,\n",
    "                        \"-ss\", str(start_time),\n",
    "                        \"-t\", str(segment_length),\n",
    "                        \"-c:v\", \"libx264\",\n",
    "                        \"-preset\", \"ultrafast\",  # Use ultrafast preset for speed\n",
    "                        \"-pix_fmt\", \"yuv420p\",   # Ensure compatible pixel format\n",
    "                        segment_path\n",
    "                    ]\n",
    "                    subprocess.run(cmd, check=True)\n",
    "                    # Process segment with both highlight sets\n",
    "                    if process_segment(processor, model, segment_path, highlights1):\n",
    "                        print(\"KEEPING SEGMENT FOR SET 1\")\n",
    "                        kept_segments1.append((start_time, end_time))\n",
    "                    if process_segment(processor, model, segment_path, highlights2):\n",
    "                        print(\"KEEPING SEGMENT FOR SET 2\")\n",
    "                        kept_segments2.append((start_time, end_time))\n",
    "                    # Clean up segment file\n",
    "                    os.remove(segment_path)\n",
    "                    segments_processed += 1\n",
    "                # Remove temp directory\n",
    "                os.rmdir(temp_dir)\n",
    "                # Calculate percentages of video kept for each highlight set\n",
    "                total_duration = duration\n",
    "                duration1 = sum(end - start for start, end in kept_segments1)\n",
    "                duration2 = sum(end - start for start, end in kept_segments2)\n",
    "                percent1 = (duration1 / total_duration) * 100\n",
    "                percent2 = (duration2 / total_duration) * 100\n",
    "                print(f\"Highlight set 1: {percent1:.1f}% of video\")\n",
    "                print(f\"Highlight set 2: {percent2:.1f}% of video\")\n",
    "                # Choose the set with lower percentage unless it's zero\n",
    "                final_segments = kept_segments2 if (0 < percent2 <= percent1 or percent1 == 0) else kept_segments1\n",
    "                # Create final video\n",
    "                if final_segments:\n",
    "                    with tempfile.NamedTemporaryFile(suffix='.mp4', delete=False) as tmp_file:\n",
    "                        temp_output = tmp_file.name\n",
    "                        concatenate_scenes(video, final_segments, temp_output)\n",
    "                    selected_set = \"2\" if final_segments == kept_segments2 else \"1\"\n",
    "                    percent_used = percent2 if final_segments == kept_segments2 else percent1\n",
    "                    completion_message = f\"Processing complete! Used highlight set {selected_set} ({percent_used:.1f}% of video)\"\n",
    "                    yield [\n",
    "                        completion_message,\n",
    "                        formatted_desc,\n",
    "                        formatted_highlights,\n",
    "                        gr.update(value=temp_output, visible=True),\n",
    "                        gr.update(visible=True)\n",
    "                    ]\n",
    "                else:\n",
    "                    yield [\n",
    "                        \"No highlights detected in the video with either set of criteria.\",\n",
    "                        formatted_desc,\n",
    "                        formatted_highlights,\n",
    "                        gr.update(visible=False),\n",
    "                        gr.update(visible=True)\n",
    "                    ]\n",
    "            except Exception as e:\n",
    "                logger.exception(\"Error processing video\")\n",
    "                yield [\n",
    "                    f\"Error processing video: {str(e)}\",\n",
    "                    \"\",\n",
    "                    \"\",\n",
    "                    gr.update(visible=False),\n",
    "                    gr.update(visible=False)\n",
    "                ]\n",
    "            finally:\n",
    "                # Clean up\n",
    "                torch.cuda.empty_cache()\n",
    "        process_btn.click(\n",
    "            on_process,\n",
    "            inputs=[input_video],\n",
    "            outputs=[\n",
    "                status,\n",
    "                video_description,\n",
    "                highlight_types,\n",
    "                output_video,\n",
    "                analysis_accordion\n",
    "            ],\n",
    "            queue=True,\n",
    "        )\n",
    "    return app\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f7ed87",
   "metadata": {},
   "source": [
    "## Launch the Gradio Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08389f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMPORTANT: You are using gradio version 3.50.0, however version 4.44.1 is available, please upgrade.\n",
      "--------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "492f2b99dbf24a1a9a7f44ad4d502708",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03eb1d44742b4405aafcb876b870c9c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cbf456c62474cdaa5d8b2717a7933c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using prompt 1 for highlight detection\n",
      "[{'role': 'system', 'content': [{'type': 'text', 'text': 'You are a highlight editor. List archetypal dramatic moments that would make compelling highlights if they appear in the video. Each moment should be specific enough to be recognizable but generic enough to potentially exist in other videos of this type.'}]}, {'role': 'user', 'content': [{'type': 'text', 'text': 'Here is a description of a video:\\n\\nthe video appears to be a demonstration of a medical device or equipment being handled by medical professionals. the individuals in the video are wearing blue lab coats and are handling various medical instruments, possibly including a stethoscope and a clipboard. the setting is a clinical or medical environment with white walls and a clean, clinical appearance.\\n\\nList potential highlight moments to look for in this video:'}]}]\n",
      "Using prompt 2 for highlight detection\n",
      "[{'role': 'system', 'content': [{'type': 'text', 'text': 'You are a helpful visual-language assistant that can understand videos and edit. You are tasked with helping the user to create highlight reels for videos. Highlights should be rare and important events in the video in question.'}]}, {'role': 'user', 'content': [{'type': 'text', 'text': 'Here is a description of a video:\\n\\nthe video appears to be a demonstration of a medical device or equipment being handled by medical professionals. the individuals in the video are wearing blue lab coats and are handling various medical instruments, possibly including a stethoscope and a clipboard. the setting is a clinical or medical environment with white walls and a clean, clinical appearance.\\n\\nList dramatic moments that would make compelling highlights if they appear in the video. Each moment should be specific enough to be recognizable but generic enough to potentially exist in any video of this type:'}]}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffmpeg version 4.4.2-0ubuntu0.22.04.1 Copyright (c) 2000-2021 the FFmpeg developers\n",
      "  built with gcc 11 (Ubuntu 11.2.0-19ubuntu1)\n",
      "  configuration: --prefix=/usr --extra-version=0ubuntu0.22.04.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librabbitmq --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-pocketsphinx --enable-librsvg --enable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
      "  libavutil      56. 70.100 / 56. 70.100\n",
      "  libavcodec     58.134.100 / 58.134.100\n",
      "  libavformat    58. 76.100 / 58. 76.100\n",
      "  libavdevice    58. 13.100 / 58. 13.100\n",
      "  libavfilter     7.110.100 /  7.110.100\n",
      "  libswscale      5.  9.100 /  5.  9.100\n",
      "  libswresample   3.  9.100 /  3.  9.100\n",
      "  libpostproc    55.  9.100 / 55.  9.100\n",
      "Input #0, mov,mp4,m4a,3gp,3g2,mj2, from '/tmp/gradio/1c729b355deb782f0fd0f22ddcc54b6b9ebf53f8/test01.mp4':\n",
      "  Metadata:\n",
      "    major_brand     : isom\n",
      "    minor_version   : 512\n",
      "    compatible_brands: isomiso2avc1mp41\n",
      "    encoder         : Lavf57.41.100\n",
      "  Duration: 00:00:10.00, start: 0.000000, bitrate: 355 kb/s\n",
      "  Stream #0:0(und): Video: h264 (High) (avc1 / 0x31637661), yuv420p, 480x270 [SAR 1:1 DAR 16:9], 225 kb/s, 15 fps, 15 tbr, 15360 tbn, 30 tbc (default)\n",
      "    Metadata:\n",
      "      handler_name    : VideoHandler\n",
      "      vendor_id       : [0][0][0][0]\n",
      "  Stream #0:1(und): Audio: aac (LC) (mp4a / 0x6134706D), 44100 Hz, stereo, fltp, 125 kb/s (default)\n",
      "    Metadata:\n",
      "      handler_name    : SoundHandler\n",
      "      vendor_id       : [0][0][0][0]\n",
      "Stream mapping:\n",
      "  Stream #0:0 -> #0:0 (h264 (native) -> h264 (libx264))\n",
      "  Stream #0:1 -> #0:1 (aac (native) -> aac (native))\n",
      "Press [q] to stop, [?] for help\n",
      "[libx264 @ 0x55f9ec93e980] using SAR=1/1\n",
      "[libx264 @ 0x55f9ec93e980] using cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2\n",
      "[libx264 @ 0x55f9ec93e980] profile Constrained Baseline, level 2.1, 4:2:0, 8-bit\n",
      "[libx264 @ 0x55f9ec93e980] 264 - core 163 r3060 5db6aa6 - H.264/MPEG-4 AVC codec - Copyleft 2003-2021 - http://www.videolan.org/x264.html - options: cabac=0 ref=1 deblock=0:0:0 analyse=0:0 me=dia subme=0 psy=1 psy_rd=1.00:0.00 mixed_ref=0 me_range=16 chroma_me=1 trellis=0 8x8dct=0 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=0 threads=8 lookahead_threads=1 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=0 weightp=0 keyint=250 keyint_min=15 scenecut=0 intra_refresh=0 rc=crf mbtree=0 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=0\n",
      "Output #0, mp4, to 'temp_segments/segment_0.mp4':\n",
      "  Metadata:\n",
      "    major_brand     : isom\n",
      "    minor_version   : 512\n",
      "    compatible_brands: isomiso2avc1mp41\n",
      "    encoder         : Lavf58.76.100\n",
      "  Stream #0:0(und): Video: h264 (avc1 / 0x31637661), yuv420p(progressive), 480x270 [SAR 1:1 DAR 16:9], q=2-31, 15 fps, 15360 tbn (default)\n",
      "    Metadata:\n",
      "      handler_name    : VideoHandler\n",
      "      vendor_id       : [0][0][0][0]\n",
      "      encoder         : Lavc58.134.100 libx264\n",
      "    Side data:\n",
      "      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: N/A\n",
      "  Stream #0:1(und): Audio: aac (LC) (mp4a / 0x6134706D), 44100 Hz, stereo, fltp, 128 kb/s (default)\n",
      "    Metadata:\n",
      "      handler_name    : SoundHandler\n",
      "      vendor_id       : [0][0][0][0]\n",
      "      encoder         : Lavc58.134.100 aac\n",
      "frame=  150 fps=0.0 q=-1.0 Lsize=     783kB time=00:00:09.97 bitrate= 642.7kbits/s speed=  70x    \n",
      "video:615kB audio:162kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.680013%\n",
      "[libx264 @ 0x55f9ec93e980] frame I:1     Avg QP:20.00  size:  9154\n",
      "[libx264 @ 0x55f9ec93e980] frame P:149   Avg QP:20.65  size:  4163\n",
      "[libx264 @ 0x55f9ec93e980] mb I  I16..4: 100.0%  0.0%  0.0%\n",
      "[libx264 @ 0x55f9ec93e980] mb P  I16..4: 10.2%  0.0%  0.0%  P16..4: 33.2%  0.0%  0.0%  0.0%  0.0%    skip:56.6%\n",
      "[libx264 @ 0x55f9ec93e980] coded y,uvDC,uvAC intra: 47.0% 60.6% 18.0% inter: 22.4% 20.5% 2.2%\n",
      "[libx264 @ 0x55f9ec93e980] i16 v,h,dc,p: 43% 28% 12% 17%\n",
      "[libx264 @ 0x55f9ec93e980] i8c dc,h,v,p: 35% 23% 30% 11%\n",
      "[libx264 @ 0x55f9ec93e980] kb/s:503.59\n",
      "[aac @ 0x55f9ec967080] Qavg: 340.478\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'system', 'content': [{'type': 'text', 'text': 'You are a video highlight analyzer. Your role is to identify moments that have high dramatic value, focusing on displays of skill, emotion, personality, or tension. Compare video segments against provided example highlights to find moments with similar emotional impact and visual interest, even if the specific actions differ.'}]}, {'role': 'user', 'content': [{'type': 'video', 'path': 'temp_segments/segment_0.mp4'}, {'type': 'text', 'text': \"Given these highlight examples:\\nHighlights:\\n\\n1. A medical professional with a blue lab coat handling a stethoscope on a patient's chest.\\n2. A medical professional taking a blood pressure reading on a patient's arm.\\n3. A medical professional using a clipboard to record patient information, such as name, age, and medical history.\\n4. A medical professional performing a procedure or examination on a patient, such as an electrocardiogram (ECG) or a physical exam.\\n5. A medical professional explaining a medical procedure or test to a patient or their family.\\n6. A medical professional working with a patient who is undergoing a medical procedure, such as an injection or a bandage application.\\n7. A medical professional handling a patient's medical chart or medical records.\\n8. A medical professional using a computer or electronic device to enter or access patient information.\\n9. A medical professional discussing a patient's medical history or test results with a patient or their family.\\n10. A medical professional demonstrating a medical device or equipment, such as a defibrillator or a ventilator.\\n11. A medical professional handling a patient's medication or supplies, such as medication bottles or medical supplies.\\n12. A medical professional using a\\n\\nDoes this video contain a moment that matches the core action of one of the highlights? Answer with:\\n'yes' or 'no'\\nIf yes, justify it\"}]}]\n",
      "Segment response yes\n",
      "KEEPING SEGMENT FOR SET 1\n",
      "[{'role': 'system', 'content': [{'type': 'text', 'text': 'You are a video highlight analyzer. Your role is to identify moments that have high dramatic value, focusing on displays of skill, emotion, personality, or tension. Compare video segments against provided example highlights to find moments with similar emotional impact and visual interest, even if the specific actions differ.'}]}, {'role': 'user', 'content': [{'type': 'video', 'path': 'temp_segments/segment_0.mp4'}, {'type': 'text', 'text': \"Given these highlight examples:\\n1. The medical professionals are handling a vital equipment, like an IV or a blood pressure cuff, with precision and care, showcasing the importance of their work.\\n\\n2. A medical professional performs an examination on a patient, using the stethoscope to listen to the heart and lungs, highlighting the importance of a thorough medical check-up.\\n\\n3. Medical instruments are being used to extract a sample from a patient, demonstrating the procedures involved in diagnosing and treating medical conditions.\\n\\n4. A medical professional is using a computer terminal to input data, indicating the use of technology in modern medical practices.\\n\\n5. A medical professional is performing a procedure, like a surgery, and the video highlights the precision and skill required in such a situation.\\n\\n6. Medical professionals are discussing a patient's case, highlighting the importance of teamwork and collaboration in medical care.\\n\\n7. A medical professional is handling a patient's medication, emphasizing the importance of proper medication administration.\\n\\n8. A medical professional is performing a blood test, showcasing the importance of accurate and timely medical testing.\\n\\n9. Medical professionals are working together to administer treatment, highlighting the importance of teamwork and communication in medical care.\\n\\n10. A medical professional is using a medical device, like a ventil\\n\\nDoes this video contain a moment that matches the core action of one of the highlights? Answer with:\\n'yes' or 'no'\\nIf yes, justify it\"}]}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Running ffmpeg command: ffmpeg -y -i /tmp/gradio/1c729b355deb782f0fd0f22ddcc54b6b9ebf53f8/test01.mp4 -filter_complex [0:v]trim=start=0:end=10.0,setpts=PTS-STARTPTS[v0];[0:a]atrim=start=0:end=10.0,asetpts=PTS-STARTPTS[a0];[v0][a0]concat=n=1:v=1:a=1[outv][outa] -map [outv] -map [outa] -c:v libx264 -c:a aac /tmp/tmp0txel9p9.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segment response no\n",
      "Highlight set 1: 100.0% of video\n",
      "Highlight set 2: 0.0% of video\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffmpeg version 4.4.2-0ubuntu0.22.04.1 Copyright (c) 2000-2021 the FFmpeg developers\n",
      "  built with gcc 11 (Ubuntu 11.2.0-19ubuntu1)\n",
      "  configuration: --prefix=/usr --extra-version=0ubuntu0.22.04.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librabbitmq --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-pocketsphinx --enable-librsvg --enable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
      "  libavutil      56. 70.100 / 56. 70.100\n",
      "  libavcodec     58.134.100 / 58.134.100\n",
      "  libavformat    58. 76.100 / 58. 76.100\n",
      "  libavdevice    58. 13.100 / 58. 13.100\n",
      "  libavfilter     7.110.100 /  7.110.100\n",
      "  libswscale      5.  9.100 /  5.  9.100\n",
      "  libswresample   3.  9.100 /  3.  9.100\n",
      "  libpostproc    55.  9.100 / 55.  9.100\n",
      "Input #0, mov,mp4,m4a,3gp,3g2,mj2, from '/tmp/gradio/1c729b355deb782f0fd0f22ddcc54b6b9ebf53f8/test01.mp4':\n",
      "  Metadata:\n",
      "    major_brand     : isom\n",
      "    minor_version   : 512\n",
      "    compatible_brands: isomiso2avc1mp41\n",
      "    encoder         : Lavf57.41.100\n",
      "  Duration: 00:00:10.00, start: 0.000000, bitrate: 355 kb/s\n",
      "  Stream #0:0(und): Video: h264 (High) (avc1 / 0x31637661), yuv420p, 480x270 [SAR 1:1 DAR 16:9], 225 kb/s, 15 fps, 15 tbr, 15360 tbn, 30 tbc (default)\n",
      "    Metadata:\n",
      "      handler_name    : VideoHandler\n",
      "      vendor_id       : [0][0][0][0]\n",
      "  Stream #0:1(und): Audio: aac (LC) (mp4a / 0x6134706D), 44100 Hz, stereo, fltp, 125 kb/s (default)\n",
      "    Metadata:\n",
      "      handler_name    : SoundHandler\n",
      "      vendor_id       : [0][0][0][0]\n",
      "Stream mapping:\n",
      "  Stream #0:0 (h264) -> trim\n",
      "  Stream #0:1 (aac) -> atrim\n",
      "  concat:out:v0 -> Stream #0:0 (libx264)\n",
      "  concat:out:a0 -> Stream #0:1 (aac)\n",
      "Press [q] to stop, [?] for help\n",
      "[libx264 @ 0x577f326b3380] using SAR=1/1\n",
      "[libx264 @ 0x577f326b3380] using cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2\n",
      "[libx264 @ 0x577f326b3380] profile High, level 2.1, 4:2:0, 8-bit\n",
      "[libx264 @ 0x577f326b3380] 264 - core 163 r3060 5db6aa6 - H.264/MPEG-4 AVC codec - Copyleft 2003-2021 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=-2 threads=8 lookahead_threads=1 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=15 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\n",
      "Output #0, mp4, to '/tmp/tmp0txel9p9.mp4':\n",
      "  Metadata:\n",
      "    major_brand     : isom\n",
      "    minor_version   : 512\n",
      "    compatible_brands: isomiso2avc1mp41\n",
      "    encoder         : Lavf58.76.100\n",
      "  Stream #0:0: Video: h264 (avc1 / 0x31637661), yuv420p(progressive), 480x270 [SAR 1:1 DAR 16:9], q=2-31, 15 fps, 15360 tbn (default)\n",
      "    Metadata:\n",
      "      encoder         : Lavc58.134.100 libx264\n",
      "    Side data:\n",
      "      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: N/A\n",
      "  Stream #0:1: Audio: aac (LC) (mp4a / 0x6134706D), 44100 Hz, stereo, fltp, 128 kb/s (default)\n",
      "    Metadata:\n",
      "      encoder         : Lavc58.134.100 aac\n",
      "frame=  150 fps=0.0 q=-1.0 Lsize=     419kB time=00:00:09.96 bitrate= 344.9kbits/s speed=16.4x    \n",
      "video:251kB audio:162kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 1.534410%\n",
      "[libx264 @ 0x577f326b3380] frame I:2     Avg QP:16.65  size:  7297\n",
      "[libx264 @ 0x577f326b3380] frame P:63    Avg QP:20.68  size:  2384\n",
      "[libx264 @ 0x577f326b3380] frame B:85    Avg QP:23.83  size:  1077\n",
      "[libx264 @ 0x577f326b3380] consecutive B-frames: 15.3% 24.0% 10.0% 50.7%\n",
      "[libx264 @ 0x577f326b3380] mb I  I16..4: 39.4% 24.7% 35.9%\n",
      "[libx264 @ 0x577f326b3380] mb P  I16..4:  3.7%  8.8%  4.0%  P16..4: 18.9%  9.1%  4.2%  0.0%  0.0%    skip:51.3%\n",
      "[libx264 @ 0x577f326b3380] mb B  I16..4:  1.1%  3.2%  1.1%  B16..8: 18.4%  5.8%  1.5%  direct: 4.5%  skip:64.4%  L0:42.8% L1:38.8% BI:18.4%\n",
      "[libx264 @ 0x577f326b3380] 8x8 transform intra:51.5% inter:57.7%\n",
      "[libx264 @ 0x577f326b3380] coded y,uvDC,uvAC intra: 51.5% 68.3% 26.5% inter: 10.0% 12.8% 0.4%\n",
      "[libx264 @ 0x577f326b3380] i16 v,h,dc,p: 51% 27%  8% 15%\n",
      "[libx264 @ 0x577f326b3380] i8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 25% 18% 22%  6%  5%  5%  5%  7%  6%\n",
      "[libx264 @ 0x577f326b3380] i4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 32% 20% 15%  6%  5%  6%  5%  6%  4%\n",
      "[libx264 @ 0x577f326b3380] i8c dc,h,v,p: 45% 20% 27%  8%\n",
      "[libx264 @ 0x577f326b3380] Weighted P-Frames: Y:0.0% UV:0.0%\n",
      "[libx264 @ 0x577f326b3380] ref P L0: 73.8% 14.8%  8.0%  3.4%\n",
      "[libx264 @ 0x577f326b3380] ref B L0: 94.5%  4.4%  1.1%\n",
      "[libx264 @ 0x577f326b3380] ref B L1: 98.6%  1.4%\n",
      "[libx264 @ 0x577f326b3380] kb/s:205.08\n",
      "[aac @ 0x577f326d3440] Qavg: 340.478\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "app = create_ui(\"HuggingFaceTB/SmolVLM2-2.2B-Instruct\")\n",
    "app.queue().launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv_playgrounds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
