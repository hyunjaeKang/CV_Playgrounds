{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a0e698b",
   "metadata": {},
   "source": [
    "# FastNeRF: High-Fidelity Neural Rendering at 200FPS\n",
    "\n",
    "---\n",
    "\n",
    "- Conda env : [3dcv_playgrounds](../README.md#setup-a-conda-environment)\n",
    "\n",
    "----\n",
    "\n",
    "- Ref: \n",
    "    - https://arxiv.org/abs/2103.10380\n",
    "    - https://github.com/MaximeVandegar/Papers-in-100-Lines-of-Code/tree/main/FastNeRF_High_Fidelity_Neural_Rendering_at_200FPS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9c4eab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchshow as ts\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7160424d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping already downloaded file ./temp/data/training_data.pkl\n",
      "Skipping already downloaded file ./temp/data/testing_data.pkl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./temp/data/testing_data.pkl'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download Data\n",
    "Path('./temp/data').mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "url = 'https://drive.google.com/uc?id=1hH7NhaXxIthO9-FeT16fvpf_MVIhf41J'\n",
    "train_data_path = './temp/data/training_data.pkl'\n",
    "gdown.download(url, train_data_path, quiet=False, resume=True)\n",
    "\n",
    "\n",
    "url = 'https://drive.google.com/uc?id=16M64h0KKgFKhM8hJDpqd15YWYhafUs2Q'\n",
    "test_data_path = './temp/data/testing_data.pkl'\n",
    "gdown.download(url, test_data_path, quiet=False, resume=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dabc8a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_model_path = './temp/01_FastNeRF_models/'\n",
    "g_video_path = './temp/01_FastNeRF_novel_views/'\n",
    "g_fps = 10\n",
    "Path(g_model_path).mkdir(exist_ok=True, parents=True)\n",
    "Path(g_video_path).mkdir(exist_ok=True, parents=True)\n",
    "g_training_dataset = torch.from_numpy(np.load('./temp/data/training_data.pkl', allow_pickle=True))\n",
    "g_testing_dataset = torch.from_numpy(np.load('./temp/data/testing_data.pkl', allow_pickle=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0552a80c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current memory allocated on MPS: 0 bytes\n",
      "Driver memory allocated on MPS: 393216 bytes\n",
      "mps\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    g_device = torch.device(\"mps\")\n",
    "    print(f\"Current memory allocated on MPS: {torch.mps.current_allocated_memory()} bytes\")\n",
    "    print(f\"Driver memory allocated on MPS: {torch.mps.driver_allocated_memory()} bytes\")\n",
    "elif torch.cuda.is_available():\n",
    "    g_device = torch.device(\"cuda\")\n",
    "else:\n",
    "    g_device = torch.device(\"cpu\")\n",
    "print(g_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8b6eefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fast NERF Model\n",
    "\n",
    "class FastNerf(nn.Module):\n",
    "    def __init__(self, embedding_dim_pos=10, embedding_dim_direction=4, hidden_dim_pos=384, hidden_dim_dir=128, D=8):\n",
    "        super(FastNerf, self).__init__()\n",
    "\n",
    "        self.Fpos = nn.Sequential(nn.Linear(embedding_dim_pos * 6 + 3, hidden_dim_pos), nn.ReLU(),\n",
    "                                  nn.Linear(hidden_dim_pos, hidden_dim_pos), nn.ReLU(),\n",
    "                                  nn.Linear(hidden_dim_pos, hidden_dim_pos), nn.ReLU(),\n",
    "                                  nn.Linear(hidden_dim_pos, hidden_dim_pos), nn.ReLU(),\n",
    "                                  nn.Linear(hidden_dim_pos, hidden_dim_pos), nn.ReLU(),\n",
    "                                  nn.Linear(hidden_dim_pos, hidden_dim_pos), nn.ReLU(),\n",
    "                                  nn.Linear(hidden_dim_pos, hidden_dim_pos), nn.ReLU(),\n",
    "                                  nn.Linear(hidden_dim_pos, 3 * D + 1), )\n",
    "\n",
    "        self.Fdir = nn.Sequential(nn.Linear(embedding_dim_direction * 6 + 3, hidden_dim_dir), nn.ReLU(),\n",
    "                                  nn.Linear(hidden_dim_dir, hidden_dim_dir), nn.ReLU(),\n",
    "                                  nn.Linear(hidden_dim_dir, hidden_dim_dir), nn.ReLU(),\n",
    "                                  nn.Linear(hidden_dim_dir, D), )\n",
    "\n",
    "        self.embedding_dim_pos = embedding_dim_pos\n",
    "        self.embedding_dim_direction = embedding_dim_direction\n",
    "        self.D = D\n",
    "\n",
    "    @staticmethod\n",
    "    def positional_encoding(x, L):\n",
    "        out = [x]\n",
    "        for j in range(L):\n",
    "            out.append(torch.sin(2 ** j * x))\n",
    "            out.append(torch.cos(2 ** j * x))\n",
    "        return torch.cat(out, dim=1)\n",
    "\n",
    "    def forward(self, o, d):\n",
    "        sigma_uvw = self.Fpos(self.positional_encoding(o, self.embedding_dim_pos))\n",
    "        sigma = torch.nn.functional.softplus(sigma_uvw[:, 0][..., None])  # [batch_size, 1]\n",
    "        uvw = torch.sigmoid(sigma_uvw[:, 1:].reshape(-1, 3, self.D))  # [batch_size, 3, D]\n",
    "\n",
    "        beta = torch.softmax(self.Fdir(self.positional_encoding(d, self.embedding_dim_direction)), -1)\n",
    "        color = (beta.unsqueeze(1) * uvw).sum(-1)  # [batch_size, 3]\n",
    "        return color, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fe5712b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cache(nn.Module):\n",
    "    def __init__(self, model, scale, device, Np, Nd):\n",
    "        super(Cache, self).__init__()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Position\n",
    "            x, y, z = torch.meshgrid([torch.linspace(-scale / 2, scale / 2, Np).to(device),\n",
    "                                      torch.linspace(-scale / 2, scale / 2, Np).to(device),\n",
    "                                      torch.linspace(-scale / 2, scale / 2, Np).to(device)])\n",
    "            xyz = torch.cat((x.reshape(-1, 1), y.reshape(-1, 1), z.reshape(-1, 1)), dim=1)\n",
    "            sigma_uvw = model.Fpos(model.positional_encoding(xyz, model.embedding_dim_pos))\n",
    "            self.sigma_uvw = sigma_uvw.reshape((Np, Np, Np, -1))\n",
    "            # Direction\n",
    "            xd, yd = torch.meshgrid([torch.linspace(-scale / 2, scale / 2, Nd).to(device),\n",
    "                                     torch.linspace(-scale / 2, scale / 2, Nd).to(device)])\n",
    "            xyz_d = torch.cat((xd.reshape(-1, 1), yd.reshape(-1, 1),\n",
    "                               torch.sqrt((1 - xd ** 2 - yd ** 2).clip(0, 1)).reshape(-1, 1)), dim=1)\n",
    "            beta = model.Fdir(model.positional_encoding(xyz_d, model.embedding_dim_direction))\n",
    "            self.beta = beta.reshape((Nd, Nd, -1))\n",
    "\n",
    "        self.scale = scale\n",
    "        self.Np = Np\n",
    "        self.Nd = Nd\n",
    "        self.D = model.D\n",
    "\n",
    "    def forward(self, x, d):\n",
    "        color = torch.zeros_like(x)\n",
    "        sigma = torch.zeros((x.shape[0], 1), device=x.device)\n",
    "\n",
    "        mask = (x[:, 0].abs() < (self.scale / 2)) & (x[:, 1].abs() < (self.scale / 2)) & (x[:, 2].abs() < (self.scale / 2))\n",
    "        # Position\n",
    "        idx = (x[mask] / (self.scale / self.Np) + self.Np / 2).long().clip(0, self.Np - 1)\n",
    "        sigma_uvw = self.sigma_uvw[idx[:, 0], idx[:, 1], idx[:, 2]]\n",
    "        # Direction\n",
    "        idx = (d[mask] * self.Nd).long().clip(0, self.Nd - 1)\n",
    "        beta = torch.softmax(self.beta[idx[:, 0], idx[:, 1]], -1)\n",
    "\n",
    "        sigma[mask] = torch.nn.functional.softplus(sigma_uvw[:, 0][..., None])  # [batch_size, 1]\n",
    "        uvw = torch.sigmoid(sigma_uvw[:, 1:].reshape(-1, 3, self.D))  # [batch_size, 3, D]\n",
    "        color[mask] = (beta.unsqueeze(1) * uvw).sum(-1)  # [batch_size, 3]\n",
    "        return color, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba60b400",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accumulated_transmittance(alphas):\n",
    "    accumulated_transmittance = torch.cumprod(alphas, 1)\n",
    "    return torch.cat((torch.ones((accumulated_transmittance.shape[0], 1), device=alphas.device),\n",
    "                      accumulated_transmittance[:, :-1]), dim=-1)\n",
    "\n",
    "\n",
    "def render_rays(nerf_model, ray_origins, ray_directions, hn=0, hf=0.5, nb_bins=192):\n",
    "    device = ray_origins.device\n",
    "    t = torch.linspace(hn, hf, nb_bins, device=device).expand(ray_origins.shape[0], nb_bins)\n",
    "    # Perturb sampling along each ray.\n",
    "    mid = (t[:, :-1] + t[:, 1:]) / 2.\n",
    "    lower = torch.cat((t[:, :1], mid), -1)\n",
    "    upper = torch.cat((mid, t[:, -1:]), -1)\n",
    "    u = torch.rand(t.shape, device=ray_origins.device)\n",
    "    t = lower + (upper - lower) * u  # [batch_size, nb_bins]\n",
    "    delta = torch.cat((t[:, 1:] - t[:, :-1], torch.tensor([1e10], device=device).expand(ray_origins.shape[0], 1)), -1)\n",
    "\n",
    "    x = ray_origins.unsqueeze(1) + t.unsqueeze(2) * ray_directions.unsqueeze(1)  # [batch_size, nb_bins, 3]\n",
    "    ray_directions = ray_directions.expand(nb_bins, ray_directions.shape[0], 3).transpose(0, 1)\n",
    "\n",
    "    colors, sigma = nerf_model(x.reshape(-1, 3), ray_directions.reshape(-1, 3))\n",
    "    colors = colors.reshape(x.shape)\n",
    "    sigma = sigma.reshape(x.shape[:-1])\n",
    "\n",
    "    alpha = 1 - torch.exp(-sigma * delta)  # [batch_size, nb_bins]\n",
    "    weights = compute_accumulated_transmittance(1 - alpha).unsqueeze(2) * alpha.unsqueeze(2)\n",
    "    c = (weights * colors).sum(dim=1)  # Pixel values\n",
    "    weight_sum = weights.sum(-1).sum(-1) # Regularization for white background\n",
    "    return c + 1 - weight_sum.unsqueeze(-1)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model, hn, hf, dataset, device = 'cpu', img_index=0, nb_bins=192, H=400, W=400):\n",
    "    ray_origins = dataset[img_index * H * W: (img_index + 1) * H * W, :3]\n",
    "    ray_directions = dataset[img_index * H * W: (img_index + 1) * H * W, 3:6]\n",
    "    regenerated_px_values = render_rays(model, ray_origins.to(device), ray_directions.to(device), hn=hn, hf=hf,\n",
    "                                        nb_bins=nb_bins)\n",
    "\n",
    "    # plt.figure()\n",
    "    # plt.imshow(regenerated_px_values.data.cpu().numpy().reshape(H, W, 3).clip(0, 1))\n",
    "    # plt.axis('off')\n",
    "    # plt.savefig(f'./temp/01_novel_views/img_{img_index}.png', bbox_inches='tight')\n",
    "    # plt.close()\n",
    "    return regenerated_px_values.cpu().reshape(H, W, 3)\n",
    "\n",
    "\n",
    "def train(nerf_model, optimizer, scheduler, data_loader, device='cpu', hn=0, hf=1, nb_epochs=int(1e5), nb_bins=192, eval_steps = 5):\n",
    "    training_loss = []\n",
    "    for e in (range(nb_epochs)):\n",
    "        print(f\"epoch : {e}\")\n",
    "        for ep, batch in enumerate(tqdm(data_loader)):\n",
    "\n",
    "            ray_origins = batch[:, :3].to(device)\n",
    "            ray_directions = batch[:, 3:6].to(device)\n",
    "            ground_truth_px_values = batch[:, 6:].to(device)\n",
    "\n",
    "            regenerated_px_values = render_rays(nerf_model, ray_origins, ray_directions, hn=hn, hf=hf, nb_bins=nb_bins)\n",
    "            loss = ((ground_truth_px_values - regenerated_px_values) ** 2).sum()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            training_loss.append(loss.item())\n",
    "        scheduler.step()\n",
    "        # torch.save(nerf_model.cpu(), 'nerf_model')\n",
    "        torch.save(nerf_model.state_dict(), f'./temp/01_models/nerf_model_{e:03d}.pth')\n",
    "        # nerf_model.to(device)\n",
    "        _model_path = os.path.join(g_model_path, f'nerf_model_{e:03d}.pth')\n",
    "        torch.save(nerf_model.state_dict(), _model_path)\n",
    "\n",
    "        if e % eval_steps == 0 or e == nb_epochs - 1:\n",
    "            nerf_model.eval()\n",
    "            imgT_lst = []\n",
    "            cache = Cache(nerf_model, 2.2, g_device, 192, 128)\n",
    "            for idx in tqdm(range(200), desc=\"Validation\"):\n",
    "                imgT_lst.append(test(cache, 2, 6, g_testing_dataset, device = g_device, img_index=idx, nb_bins=192).unsqueeze(0))\n",
    "            img_t = (torch.cat(imgT_lst) * 255).to(torch.uint8)\n",
    "            _video_path = os.path.join(g_video_path,  f'nerf_model_{e:03d}.mp4')\n",
    "            torchvision.io.write_video(_video_path, img_t, g_fps)\n",
    "            nerf_model.train()\n",
    "    return training_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46d2b215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15625/15625 [54:07<00:00,  4.81it/s]\n",
      "/Users/hyunjae.k/110_HyunJae_Git/2025_Drills/.venv/lib/python3.10/site-packages/torch/functional.py:539: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorShape.cpp:3638.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "Validation: 100%|██████████| 200/200 [01:46<00:00,  1.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15625/15625 [55:09<00:00,  4.72it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15625/15625 [1:00:51<00:00,  4.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15625/15625 [55:42<00:00,  4.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15625/15625 [55:49<00:00,  4.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15625/15625 [57:57<00:00,  4.49it/s]\n",
      "Validation: 100%|██████████| 200/200 [02:14<00:00,  1.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15625/15625 [1:01:32<00:00,  4.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15625/15625 [1:01:23<00:00,  4.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15625/15625 [1:00:28<00:00,  4.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15625/15625 [56:37<00:00,  4.60it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15625/15625 [54:14<00:00,  4.80it/s]\n",
      "Validation: 100%|██████████| 200/200 [01:46<00:00,  1.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15625/15625 [55:54<00:00,  4.66it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15625/15625 [1:02:01<00:00,  4.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15625/15625 [59:34<00:00,  4.37it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15625/15625 [56:57<00:00,  4.57it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15625/15625 [55:46<00:00,  4.67it/s] \n",
      "Validation: 100%|██████████| 200/200 [01:31<00:00,  2.19it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model = FastNerf().to(g_device)\n",
    "model_optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(model_optimizer, milestones=[2, 4, 8], gamma=0.5)\n",
    "\n",
    "data_loader = DataLoader(g_training_dataset, batch_size=1024, shuffle=True)\n",
    "train(model, model_optimizer, scheduler, data_loader, nb_epochs=16, device=g_device, hn=2, hf=6)\n",
    "\n",
    "cache = Cache(model, 2.2, g_device, 192, 128)\n",
    "for idx in range(200):\n",
    "    test(cache, 2., 6., g_testing_dataset, g_device, img_index=idx, nb_bins=192, H=400, W=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20bc12aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cache = Cache(model, 2.2, g_device, 192, 128)\n",
    "# imgT_lst = []\n",
    "# for idx in range(3):\n",
    "#     t =  test(cache, 2., 6., g_testing_dataset, g_device, img_index=idx, nb_bins=192, H=400, W=400)\n",
    "#     imgT_lst.append(t)\n",
    "# img_t = (torch.cat(imgT_lst) * 255).to(torch.uint8)\n",
    "# _video_path = os.path.join(g_video_path,  f'ntest.mp4')\n",
    "# torchvision.io.write_video(_video_path, img_t, g_fps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed64fcdb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
