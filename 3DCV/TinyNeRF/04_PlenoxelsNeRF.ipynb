{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a0e698b",
   "metadata": {},
   "source": [
    "# PlenOctrees for Real-time Rendering of Neural Radiance Fields\n",
    "\n",
    "- Ref: \n",
    "    - https://arxiv.org/abs/2103.14024\n",
    "    - https://github.com/MaximeVandegar/Papers-in-100-Lines-of-Code/tree/main/PlenOctrees_for_Real_time_Rendering_of_Neural_Radiance_Fields\n",
    "\n",
    "- Conda env : [gsplat](../gsplat/README.md#setup-a-conda-environment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9c4eab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchshow as ts\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7160424d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping already downloaded file ./temp/data/training_data.pkl\n",
      "Skipping already downloaded file ./temp/data/testing_data.pkl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./temp/data/testing_data.pkl'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download Data\n",
    "Path('./temp/data').mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "url = 'https://drive.google.com/uc?id=1hH7NhaXxIthO9-FeT16fvpf_MVIhf41J'\n",
    "train_data_path = './temp/data/training_data.pkl'\n",
    "gdown.download(url, train_data_path, quiet=False, resume=True)\n",
    "\n",
    "\n",
    "url = 'https://drive.google.com/uc?id=16M64h0KKgFKhM8hJDpqd15YWYhafUs2Q'\n",
    "test_data_path = './temp/data/testing_data.pkl'\n",
    "gdown.download(url, test_data_path, quiet=False, resume=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12c95d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_model_path = './temp/04_PlenoxelsNeRF_models/'\n",
    "g_video_path = './temp/04_PlenoxelsNeRF_novel_views/'\n",
    "fps = 10\n",
    "Path(g_model_path).mkdir(exist_ok=True, parents=True)\n",
    "Path(g_video_path).mkdir(exist_ok=True, parents=True)\n",
    "training_dataset = torch.from_numpy(np.load('./temp/data/training_data.pkl', allow_pickle=True))\n",
    "testing_dataset = torch.from_numpy(np.load('./temp/data/testing_data.pkl', allow_pickle=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0552a80c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    g_device = torch.device(\"mps\")\n",
    "    print(f\"Current memory allocated on MPS: {torch.mps.current_allocated_memory()} bytes\")\n",
    "    print(f\"Driver memory allocated on MPS: {torch.mps.driver_allocated_memory()} bytes\")\n",
    "elif torch.cuda.is_available():\n",
    "    g_device = torch.device(\"cuda\")\n",
    "else:\n",
    "    g_device = torch.device(\"cpu\")\n",
    "print(g_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8b6eefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "\n",
    "def eval_spherical_function(k, d):\n",
    "    x, y, z = d[..., 0:1], d[..., 1:2], d[..., 2:3]\n",
    "\n",
    "    # Modified from https://github.com/google/spherical-harmonics/blob/master/sh/spherical_harmonics.cc\n",
    "    return 0.282095 * k[..., 0] + \\\n",
    "        - 0.488603 * y * k[..., 1] + 0.488603 * z * k[..., 2] - 0.488603 * x * k[..., 3] + \\\n",
    "        (1.092548 * x * y * k[..., 4] - 1.092548 * y * z * k[..., 5] + 0.315392 * (2.0 * z * z - x * x - y * y) * k[\n",
    "               ..., 6] + -1.092548 * x * z * k[..., 7] + 0.546274 * (x * x - y * y) * k[..., 8])\n",
    "\n",
    "\n",
    "class NerfModel(nn.Module):\n",
    "    def __init__(self, N=256, scale=1.5):\n",
    "        \"\"\"\n",
    "        :param N\n",
    "        :param scale: The maximum absolute value among all coordinates for objects in the scene\n",
    "        \"\"\"\n",
    "        super(NerfModel, self).__init__()\n",
    "\n",
    "        self.voxel_grid = nn.Parameter(torch.ones((N, N, N, 27 + 1)) / 100)\n",
    "        self.scale = scale\n",
    "        self.N = N\n",
    "\n",
    "    def forward(self, x, d):\n",
    "        color = torch.zeros_like(x)\n",
    "        sigma = torch.zeros((x.shape[0]), device=x.device)\n",
    "        mask = (x[:, 0].abs() < self.scale) & (x[:, 1].abs() < self.scale) & (x[:, 2].abs() < self.scale)\n",
    "\n",
    "        idx = (x[mask] / (2 * self.scale / self.N) + self.N / 2).long().clip(0, self.N - 1)\n",
    "        tmp = self.voxel_grid[idx[:, 0], idx[:, 1], idx[:, 2]]\n",
    "        sigma[mask], k = torch.nn.functional.relu(tmp[:, 0]), tmp[:, 1:]\n",
    "        color[mask] = eval_spherical_function(k.reshape(-1, 3, 9), d[mask])\n",
    "        return color, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fe5712b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rendering\n",
    "\n",
    "def compute_accumulated_transmittance(alphas):\n",
    "    accumulated_transmittance = torch.cumprod(alphas, 1)\n",
    "    return torch.cat((torch.ones((accumulated_transmittance.shape[0], 1), device=alphas.device),\n",
    "                      accumulated_transmittance[:, :-1]), dim=-1)\n",
    "\n",
    "\n",
    "def render_rays(nerf_model, ray_origins, ray_directions, hn=0, hf=0.5, nb_bins=192):\n",
    "    _device = ray_origins.device\n",
    "    t = torch.linspace(hn, hf, nb_bins, device=_device).expand(ray_origins.shape[0], nb_bins)\n",
    "    # Perturb sampling along each ray.\n",
    "    mid = (t[:, :-1] + t[:, 1:]) / 2.\n",
    "    lower = torch.cat((t[:, :1], mid), -1)\n",
    "    upper = torch.cat((mid, t[:, -1:]), -1)\n",
    "    u = torch.rand(t.shape, device=_device)\n",
    "    t = lower + (upper - lower) * u  # [batch_size, nb_bins]\n",
    "    delta = torch.cat((t[:, 1:] - t[:, :-1], torch.tensor([1e10], device=_device).expand(ray_origins.shape[0], 1)), -1)\n",
    "\n",
    "    x = ray_origins.unsqueeze(1) + t.unsqueeze(2) * ray_directions.unsqueeze(1)  # [batch_size, nb_bins, 3]\n",
    "    ray_directions = ray_directions.expand(nb_bins, ray_directions.shape[0], 3).transpose(0, 1)\n",
    "\n",
    "    colors, sigma = nerf_model(x.reshape(-1, 3), ray_directions.reshape(-1, 3))\n",
    "    colors = colors.reshape(x.shape)\n",
    "    sigma = sigma.reshape(x.shape[:-1])\n",
    "\n",
    "    alpha = 1 - torch.exp(-sigma * delta)  # [batch_size, nb_bins]\n",
    "    weights = compute_accumulated_transmittance(1 - alpha).unsqueeze(2) * alpha.unsqueeze(2)\n",
    "    c = (weights * colors).sum(dim=1)  # Pixel values\n",
    "    # Regularization for white background\n",
    "    weight_sum = weights.sum(-1).sum(-1)\n",
    "    return c + 1 - weight_sum.unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba60b400",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test(nerf_model, hn, hf, dataset, chunk_size=5, img_index=0, nb_bins=192, H=400, W=400):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        hn: near plane distance\n",
    "        hf: far plane distance\n",
    "        dataset: dataset to render\n",
    "        chunk_size (int, optional): chunk size for memory efficiency. Defaults to 10.\n",
    "        img_index (int, optional): image index to render. Defaults to 0.\n",
    "        nb_bins (int, optional): number of bins for density estimation. Defaults to 192.\n",
    "        H (int, optional): image height. Defaults to 400.\n",
    "        W (int, optional): image width. Defaults to 400.\n",
    "\n",
    "    Returns:\n",
    "        None: None\n",
    "    \"\"\"\n",
    "    ray_origins = dataset[img_index * H * W: (img_index + 1) * H * W, :3]\n",
    "    ray_directions = dataset[img_index * H * W: (img_index + 1) * H * W, 3:6]\n",
    "\n",
    "    data = []\n",
    "    for i in range(int(np.ceil(H / chunk_size))):\n",
    "        ray_origins_ = ray_origins[i * W * chunk_size: (i + 1) * W * chunk_size].to(g_device)\n",
    "        ray_directions_ = ray_directions[i * W * chunk_size: (i + 1) * W * chunk_size].to(g_device)\n",
    "        regenerated_px_values = render_rays(nerf_model, ray_origins_, ray_directions_, hn=hn, hf=hf, nb_bins=nb_bins)\n",
    "        data.append(regenerated_px_values.cpu())\n",
    "    img_t = torch.cat(data).reshape(H, W, 3)\n",
    "    return img_t\n",
    "\n",
    "def train(nerf_model, optimizer, scheduler, data_loader, device='cpu', hn=0, hf=1, start_epoch = 0, nb_epochs=int(1e5), nb_bins=192, H=400, W=400, eval_steps = 5):\n",
    "    training_loss = []\n",
    "    for e in (range(start_epoch, nb_epochs)):\n",
    "        print(f\"{e:03d}-epoch\")\n",
    "        for ep, batch in enumerate(tqdm(data_loader, total = len(data_loader), desc=\"Training\")):\n",
    "            ray_origins = batch[:, :3].to(device)\n",
    "            ray_directions = batch[:, 3:6].to(device)\n",
    "            ground_truth_px_values = batch[:, 6:].to(device)\n",
    "\n",
    "            regenerated_px_values = render_rays(nerf_model, ray_origins, ray_directions, hn=hn, hf=hf, nb_bins=nb_bins)\n",
    "            # loss = ((ground_truth_px_values - regenerated_px_values) ** 2).sum()\n",
    "            loss = torch.nn.functional.mse_loss(ground_truth_px_values, regenerated_px_values)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            training_loss.append(loss.item())\n",
    "        scheduler.step()\n",
    "        _model_path = os.path.join(g_model_path, f'nerf_model_{e:03d}.pth')\n",
    "        torch.save(nerf_model.state_dict(), _model_path)\n",
    "        # Export in TorchScript Format\n",
    "        # _model_script_path = os.path.join(g_model_path, f'nerf_model_{e:03d}.pt')\n",
    "        # _model_scripted = torch.jit.script(nerf_model)\n",
    "        # _model_scripted.save(_model_script_path)\n",
    "\n",
    "        if e % eval_steps == 0 or e == nb_epochs - 1:\n",
    "            nerf_model.eval()\n",
    "            imgT_lst = []\n",
    "            for idx in tqdm(range(200), desc=\"Validation\"):\n",
    "                imgT_lst.append(test(nerf_model, 2, 6, testing_dataset, chunk_size = 10, img_index=idx, nb_bins=192).unsqueeze(0))\n",
    "            img_t = (torch.cat(imgT_lst) * 255).to(torch.uint8)\n",
    "            _video_path = os.path.join(g_video_path,  f'nerf_model_{e:03d}.mp4')\n",
    "            torchvision.io.write_video(_video_path, img_t, fps)\n",
    "            nerf_model.train()\n",
    "\n",
    "    return training_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46d2b215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000-epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 15625/15625 [23:38<00:00, 11.01it/s]\n",
      "Validation: 100%|██████████| 200/200 [00:31<00:00,  6.42it/s]\n",
      "/home/hyunjae/anaconda3/envs/gsplat/lib/python3.10/site-packages/torchvision/io/_video_deprecation_warning.py:5: UserWarning: The video decoding and encoding capabilities of torchvision are deprecated from version 0.22 and will be removed in version 0.24. We recommend that you migrate to TorchCodec, where we'll consolidate the future decoding/encoding capabilities of PyTorch: https://github.com/pytorch/torchcodec\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "001-epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 15625/15625 [23:43<00:00, 10.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "002-epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 15625/15625 [23:31<00:00, 11.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "003-epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 15625/15625 [23:35<00:00, 11.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "004-epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 15625/15625 [23:33<00:00, 11.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "005-epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 15625/15625 [23:14<00:00, 11.21it/s]\n",
      "Validation: 100%|██████████| 200/200 [00:31<00:00,  6.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "006-epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 15625/15625 [23:24<00:00, 11.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "007-epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 15625/15625 [23:36<00:00, 11.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "008-epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 15625/15625 [23:46<00:00, 10.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "009-epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 15625/15625 [23:03<00:00, 11.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "010-epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 15625/15625 [23:25<00:00, 11.12it/s]\n",
      "Validation: 100%|██████████| 200/200 [00:30<00:00,  6.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "011-epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 15625/15625 [22:58<00:00, 11.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "012-epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 15625/15625 [23:15<00:00, 11.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "013-epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 15625/15625 [23:32<00:00, 11.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "014-epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 15625/15625 [23:13<00:00, 11.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "015-epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 15625/15625 [23:16<00:00, 11.19it/s]\n",
      "Validation: 100%|██████████| 200/200 [00:30<00:00,  6.49it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.11428478360176086,\n",
       " 0.12294070422649384,\n",
       " 0.11439039558172226,\n",
       " 0.11697125434875488,\n",
       " 0.11787595599889755,\n",
       " 0.1270960569381714,\n",
       " 0.10222254693508148,\n",
       " 0.11826828122138977,\n",
       " 0.10894956439733505,\n",
       " 0.11328887939453125,\n",
       " 0.118632011115551,\n",
       " 0.1247558444738388,\n",
       " 0.1132109984755516,\n",
       " 0.11685481667518616,\n",
       " 0.10832562297582626,\n",
       " 0.11279235780239105,\n",
       " 0.10979463905096054,\n",
       " 0.11694979667663574,\n",
       " 0.1235223263502121,\n",
       " 0.10752766579389572,\n",
       " 0.10198225826025009,\n",
       " 0.1278604120016098,\n",
       " 0.11903528869152069,\n",
       " 0.10467179864645004,\n",
       " 0.1050359457731247,\n",
       " 0.11044903099536896,\n",
       " 0.11408694833517075,\n",
       " 0.1238025650382042,\n",
       " 0.12612739205360413,\n",
       " 0.12009876221418381,\n",
       " 0.10604840517044067,\n",
       " 0.12086611241102219,\n",
       " 0.10699339956045151,\n",
       " 0.1180793046951294,\n",
       " 0.10782232135534286,\n",
       " 0.11351277679204941,\n",
       " 0.09696772694587708,\n",
       " 0.10897120088338852,\n",
       " 0.1154244989156723,\n",
       " 0.11695188283920288,\n",
       " 0.11191485822200775,\n",
       " 0.1065572053194046,\n",
       " 0.11813395470380783,\n",
       " 0.11952485889196396,\n",
       " 0.11418783664703369,\n",
       " 0.10835298895835876,\n",
       " 0.10708915442228317,\n",
       " 0.11772187799215317,\n",
       " 0.12770873308181763,\n",
       " 0.11366739124059677,\n",
       " 0.10794635862112045,\n",
       " 0.09928848594427109,\n",
       " 0.11486347019672394,\n",
       " 0.11457407474517822,\n",
       " 0.1133965477347374,\n",
       " 0.10864697396755219,\n",
       " 0.11567936837673187,\n",
       " 0.12902164459228516,\n",
       " 0.11318671703338623,\n",
       " 0.11385809630155563,\n",
       " 0.11279815435409546,\n",
       " 0.10938338935375214,\n",
       " 0.11448192596435547,\n",
       " 0.10663257539272308,\n",
       " 0.09539888799190521,\n",
       " 0.10893099009990692,\n",
       " 0.11845546960830688,\n",
       " 0.13007956743240356,\n",
       " 0.11900701373815536,\n",
       " 0.11109520494937897,\n",
       " 0.10790586471557617,\n",
       " 0.10573987662792206,\n",
       " 0.1284092664718628,\n",
       " 0.11142235994338989,\n",
       " 0.12117379903793335,\n",
       " 0.10726511478424072,\n",
       " 0.11273016780614853,\n",
       " 0.11077871173620224,\n",
       " 0.1121753677725792,\n",
       " 0.12521009147167206,\n",
       " 0.1129147931933403,\n",
       " 0.12034844607114792,\n",
       " 0.10847592353820801,\n",
       " 0.11680420488119125,\n",
       " 0.13417372107505798,\n",
       " 0.10528692603111267,\n",
       " 0.11234301328659058,\n",
       " 0.118720144033432,\n",
       " 0.11190974712371826,\n",
       " 0.11868391931056976,\n",
       " 0.1217515766620636,\n",
       " 0.10441401600837708,\n",
       " 0.11937777698040009,\n",
       " 0.11838080734014511,\n",
       " 0.10582040250301361,\n",
       " 0.11907925456762314,\n",
       " 0.11687546223402023,\n",
       " 0.10201787948608398,\n",
       " 0.10551005601882935,\n",
       " 0.10395927727222443,\n",
       " 0.10381542146205902,\n",
       " 0.12092223018407822,\n",
       " 0.11140912771224976,\n",
       " 0.11601565778255463,\n",
       " 0.11553633213043213,\n",
       " 0.11719247698783875,\n",
       " 0.10938074439764023,\n",
       " 0.11846401542425156,\n",
       " 0.10647444427013397,\n",
       " 0.11276650428771973,\n",
       " 0.11192687600851059,\n",
       " 0.11853023618459702,\n",
       " 0.11034975945949554,\n",
       " 0.11386017501354218,\n",
       " 0.12176396697759628,\n",
       " 0.12118762731552124,\n",
       " 0.10650885105133057,\n",
       " 0.11125167459249496,\n",
       " 0.12003516405820847,\n",
       " 0.12142282724380493,\n",
       " 0.11708661913871765,\n",
       " 0.10147675126791,\n",
       " 0.10619845241308212,\n",
       " 0.11471061408519745,\n",
       " 0.11498710513114929,\n",
       " 0.10647173225879669,\n",
       " 0.1233639121055603,\n",
       " 0.11547412723302841,\n",
       " 0.10876022279262543,\n",
       " 0.1036137267947197,\n",
       " 0.11450475454330444,\n",
       " 0.12259986251592636,\n",
       " 0.11638851463794708,\n",
       " 0.11511200666427612,\n",
       " 0.10966560244560242,\n",
       " 0.11239282786846161,\n",
       " 0.11371991038322449,\n",
       " 0.1112959235906601,\n",
       " 0.1142328605055809,\n",
       " 0.12390195578336716,\n",
       " 0.11436079442501068,\n",
       " 0.10746894776821136,\n",
       " 0.12099134922027588,\n",
       " 0.11608758568763733,\n",
       " 0.1162424311041832,\n",
       " 0.11083819717168808,\n",
       " 0.10876114666461945,\n",
       " 0.11605550348758698,\n",
       " 0.09716682881116867,\n",
       " 0.1091257780790329,\n",
       " 0.10435257852077484,\n",
       " 0.1253315806388855,\n",
       " 0.1144031286239624,\n",
       " 0.12762832641601562,\n",
       " 0.12311627715826035,\n",
       " 0.11017048358917236,\n",
       " 0.1211506724357605,\n",
       " 0.13335689902305603,\n",
       " 0.12186476588249207,\n",
       " 0.11582358926534653,\n",
       " 0.12496324628591537,\n",
       " 0.10991574823856354,\n",
       " 0.11134286969900131,\n",
       " 0.12042050808668137,\n",
       " 0.10683860629796982,\n",
       " 0.10356518626213074,\n",
       " 0.10957539081573486,\n",
       " 0.11537988483905792,\n",
       " 0.11372479051351547,\n",
       " 0.09455196559429169,\n",
       " 0.10290016978979111,\n",
       " 0.10768024623394012,\n",
       " 0.10631432384252548,\n",
       " 0.10730808973312378,\n",
       " 0.1197979673743248,\n",
       " 0.12004237622022629,\n",
       " 0.11238537728786469,\n",
       " 0.10322075337171555,\n",
       " 0.11954426765441895,\n",
       " 0.11469807475805283,\n",
       " 0.12064647674560547,\n",
       " 0.11263877153396606,\n",
       " 0.11712092161178589,\n",
       " 0.11884689331054688,\n",
       " 0.11994382739067078,\n",
       " 0.12374626100063324,\n",
       " 0.10997724533081055,\n",
       " 0.10929934680461884,\n",
       " 0.11334674060344696,\n",
       " 0.10778379440307617,\n",
       " 0.11264097690582275,\n",
       " 0.11746621131896973,\n",
       " 0.10674703121185303,\n",
       " 0.13205642998218536,\n",
       " 0.12330039590597153,\n",
       " 0.10652504861354828,\n",
       " 0.10914125293493271,\n",
       " 0.11143200099468231,\n",
       " 0.10947838425636292,\n",
       " 0.12056407332420349,\n",
       " 0.1133003681898117,\n",
       " 0.11905652284622192,\n",
       " 0.11340342462062836,\n",
       " 0.10985376685857773,\n",
       " 0.11090339720249176,\n",
       " 0.12130822986364365,\n",
       " 0.11434066295623779,\n",
       " 0.12145420163869858,\n",
       " 0.10955001413822174,\n",
       " 0.1285969614982605,\n",
       " 0.10033323615789413,\n",
       " 0.11959327757358551,\n",
       " 0.11398782581090927,\n",
       " 0.11697861552238464,\n",
       " 0.11331915110349655,\n",
       " 0.12245379388332367,\n",
       " 0.11073964089155197,\n",
       " 0.11891984194517136,\n",
       " 0.10798758268356323,\n",
       " 0.11023908108472824,\n",
       " 0.11854734271764755,\n",
       " 0.1097022220492363,\n",
       " 0.11357975006103516,\n",
       " 0.11868119239807129,\n",
       " 0.10777445882558823,\n",
       " 0.10990075767040253,\n",
       " 0.12140842527151108,\n",
       " 0.11564844846725464,\n",
       " 0.11330687999725342,\n",
       " 0.11328135430812836,\n",
       " 0.11443684995174408,\n",
       " 0.10685082525014877,\n",
       " 0.11116808652877808,\n",
       " 0.11627733707427979,\n",
       " 0.10588623583316803,\n",
       " 0.12239791452884674,\n",
       " 0.11546419560909271,\n",
       " 0.09899979084730148,\n",
       " 0.10141738504171371,\n",
       " 0.11296485364437103,\n",
       " 0.11751165241003036,\n",
       " 0.10998257249593735,\n",
       " 0.11891063302755356,\n",
       " 0.10702449083328247,\n",
       " 0.11883784830570221,\n",
       " 0.11973980069160461,\n",
       " 0.11339420080184937,\n",
       " 0.11339251697063446,\n",
       " 0.11098414659500122,\n",
       " 0.10911399126052856,\n",
       " 0.10589933395385742,\n",
       " 0.1036311537027359,\n",
       " 0.1232425719499588,\n",
       " 0.1177784651517868,\n",
       " 0.12015925347805023,\n",
       " 0.12146499007940292,\n",
       " 0.10460826009511948,\n",
       " 0.12152056396007538,\n",
       " 0.10456084460020065,\n",
       " 0.11516072601079941,\n",
       " 0.10367973893880844,\n",
       " 0.11022976040840149,\n",
       " 0.11909618228673935,\n",
       " 0.11993461847305298,\n",
       " 0.10910021513700485,\n",
       " 0.1168297752737999,\n",
       " 0.11507084965705872,\n",
       " 0.0997229665517807,\n",
       " 0.11135701835155487,\n",
       " 0.11463163793087006,\n",
       " 0.11523394286632538,\n",
       " 0.11056777089834213,\n",
       " 0.11804439127445221,\n",
       " 0.109891876578331,\n",
       " 0.11597037315368652,\n",
       " 0.10890762507915497,\n",
       " 0.1257706880569458,\n",
       " 0.11369121074676514,\n",
       " 0.10996168851852417,\n",
       " 0.11310470104217529,\n",
       " 0.11810101568698883,\n",
       " 0.11619727313518524,\n",
       " 0.12760895490646362,\n",
       " 0.11804668605327606,\n",
       " 0.11594673246145248,\n",
       " 0.11130337417125702,\n",
       " 0.11902344226837158,\n",
       " 0.11728298664093018,\n",
       " 0.11159259080886841,\n",
       " 0.11290132254362106,\n",
       " 0.11513631790876389,\n",
       " 0.11538641154766083,\n",
       " 0.11405434459447861,\n",
       " 0.11768441647291183,\n",
       " 0.11645772308111191,\n",
       " 0.10964502394199371,\n",
       " 0.10894190520048141,\n",
       " 0.1151946634054184,\n",
       " 0.10017652809619904,\n",
       " 0.10871821641921997,\n",
       " 0.1125611960887909,\n",
       " 0.11435644328594208,\n",
       " 0.11050432175397873,\n",
       " 0.11235854029655457,\n",
       " 0.1169271394610405,\n",
       " 0.10839211940765381,\n",
       " 0.11746019124984741,\n",
       " 0.10790847986936569,\n",
       " 0.11676155030727386,\n",
       " 0.1041375994682312,\n",
       " 0.11257114261388779,\n",
       " 0.12466727197170258,\n",
       " 0.11992751806974411,\n",
       " 0.10950596630573273,\n",
       " 0.11699743568897247,\n",
       " 0.11287371814250946,\n",
       " 0.10811170190572739,\n",
       " 0.11714868247509003,\n",
       " 0.1036711037158966,\n",
       " 0.11666335165500641,\n",
       " 0.10215979069471359,\n",
       " 0.11470203101634979,\n",
       " 0.11423905938863754,\n",
       " 0.12544937431812286,\n",
       " 0.11655884981155396,\n",
       " 0.11010652780532837,\n",
       " 0.10485466569662094,\n",
       " 0.12086135149002075,\n",
       " 0.11444523185491562,\n",
       " 0.11158948391675949,\n",
       " 0.11082544177770615,\n",
       " 0.11754541099071503,\n",
       " 0.11346368491649628,\n",
       " 0.10805543512105942,\n",
       " 0.10332472622394562,\n",
       " 0.11585777997970581,\n",
       " 0.10984763503074646,\n",
       " 0.10728239268064499,\n",
       " 0.10723845660686493,\n",
       " 0.10781209915876389,\n",
       " 0.11533378064632416,\n",
       " 0.10325001925230026,\n",
       " 0.11329273879528046,\n",
       " 0.11652746796607971,\n",
       " 0.11232772469520569,\n",
       " 0.11986134946346283,\n",
       " 0.12164666503667831,\n",
       " 0.10418486595153809,\n",
       " 0.10577821731567383,\n",
       " 0.11482539027929306,\n",
       " 0.11689330637454987,\n",
       " 0.09819253534078598,\n",
       " 0.11159716546535492,\n",
       " 0.1132718175649643,\n",
       " 0.11247038841247559,\n",
       " 0.11335828900337219,\n",
       " 0.12440691888332367,\n",
       " 0.11429004371166229,\n",
       " 0.11298834532499313,\n",
       " 0.10794590413570404,\n",
       " 0.10412371158599854,\n",
       " 0.11780630052089691,\n",
       " 0.11622625589370728,\n",
       " 0.10967530310153961,\n",
       " 0.11587561666965485,\n",
       " 0.11819448322057724,\n",
       " 0.1041581854224205,\n",
       " 0.10243257135152817,\n",
       " 0.11289435625076294,\n",
       " 0.10919194668531418,\n",
       " 0.11144834756851196,\n",
       " 0.11174609512090683,\n",
       " 0.11480197310447693,\n",
       " 0.1068921685218811,\n",
       " 0.10578664392232895,\n",
       " 0.12156625092029572,\n",
       " 0.11415974795818329,\n",
       " 0.10770358890295029,\n",
       " 0.10970017313957214,\n",
       " 0.11286097764968872,\n",
       " 0.11034674942493439,\n",
       " 0.11779128015041351,\n",
       " 0.11021393537521362,\n",
       " 0.11687885969877243,\n",
       " 0.1116122305393219,\n",
       " 0.10775228589773178,\n",
       " 0.11342698335647583,\n",
       " 0.09736894816160202,\n",
       " 0.1101679801940918,\n",
       " 0.11229193210601807,\n",
       " 0.09858255833387375,\n",
       " 0.10626746714115143,\n",
       " 0.10846228897571564,\n",
       " 0.10767530649900436,\n",
       " 0.11303304135799408,\n",
       " 0.10446657985448837,\n",
       " 0.12126147001981735,\n",
       " 0.10008209943771362,\n",
       " 0.10704176872968674,\n",
       " 0.11092302203178406,\n",
       " 0.11592334508895874,\n",
       " 0.11422350257635117,\n",
       " 0.10718580335378647,\n",
       " 0.10911180824041367,\n",
       " 0.11668960750102997,\n",
       " 0.10496124625205994,\n",
       " 0.11248674243688583,\n",
       " 0.10936536639928818,\n",
       " 0.10500763356685638,\n",
       " 0.11463960260152817,\n",
       " 0.11854799091815948,\n",
       " 0.10873596370220184,\n",
       " 0.09958596527576447,\n",
       " 0.11503052711486816,\n",
       " 0.10465763509273529,\n",
       " 0.11207328736782074,\n",
       " 0.1216663271188736,\n",
       " 0.11171984672546387,\n",
       " 0.10999254882335663,\n",
       " 0.09999261796474457,\n",
       " 0.11249563097953796,\n",
       " 0.11697544157505035,\n",
       " 0.10738669335842133,\n",
       " 0.11989721655845642,\n",
       " 0.09947089850902557,\n",
       " 0.11909288167953491,\n",
       " 0.10972410440444946,\n",
       " 0.1143193244934082,\n",
       " 0.12688741087913513,\n",
       " 0.10070657730102539,\n",
       " 0.11196142435073853,\n",
       " 0.11209674924612045,\n",
       " 0.11475404351949692,\n",
       " 0.11397665739059448,\n",
       " 0.10053253173828125,\n",
       " 0.10948033630847931,\n",
       " 0.11644372344017029,\n",
       " 0.12618091702461243,\n",
       " 0.1203262209892273,\n",
       " 0.11827962845563889,\n",
       " 0.11741822957992554,\n",
       " 0.11111538112163544,\n",
       " 0.11184518039226532,\n",
       " 0.11784376949071884,\n",
       " 0.11222117394208908,\n",
       " 0.11500032246112823,\n",
       " 0.10420753806829453,\n",
       " 0.09717031568288803,\n",
       " 0.11151647567749023,\n",
       " 0.10462044924497604,\n",
       " 0.1177021861076355,\n",
       " 0.10570426285266876,\n",
       " 0.10334877669811249,\n",
       " 0.1206883043050766,\n",
       " 0.11639295518398285,\n",
       " 0.1000906452536583,\n",
       " 0.11802422255277634,\n",
       " 0.1090531200170517,\n",
       " 0.10645389556884766,\n",
       " 0.1090206652879715,\n",
       " 0.11119958013296127,\n",
       " 0.10274291038513184,\n",
       " 0.12343402951955795,\n",
       " 0.10783044993877411,\n",
       " 0.111716628074646,\n",
       " 0.10513576120138168,\n",
       " 0.09504445642232895,\n",
       " 0.10559350252151489,\n",
       " 0.1082276925444603,\n",
       " 0.11188153922557831,\n",
       " 0.11395140737295151,\n",
       " 0.11004762351512909,\n",
       " 0.10795288532972336,\n",
       " 0.11567533016204834,\n",
       " 0.11159980297088623,\n",
       " 0.11484728753566742,\n",
       " 0.11218178272247314,\n",
       " 0.09153272956609726,\n",
       " 0.10434070229530334,\n",
       " 0.11716866493225098,\n",
       " 0.11914049088954926,\n",
       " 0.12108023464679718,\n",
       " 0.10547937452793121,\n",
       " 0.11170604079961777,\n",
       " 0.1069389209151268,\n",
       " 0.10031533241271973,\n",
       " 0.11250323057174683,\n",
       " 0.1235848069190979,\n",
       " 0.12085195630788803,\n",
       " 0.10771507024765015,\n",
       " 0.11103799939155579,\n",
       " 0.10851149260997772,\n",
       " 0.10956715047359467,\n",
       " 0.09638015925884247,\n",
       " 0.11100402474403381,\n",
       " 0.10287805646657944,\n",
       " 0.10632705688476562,\n",
       " 0.11081394553184509,\n",
       " 0.11027856171131134,\n",
       " 0.10982804000377655,\n",
       " 0.10847699642181396,\n",
       " 0.10731960833072662,\n",
       " 0.1003713607788086,\n",
       " 0.1042565107345581,\n",
       " 0.11202611774206161,\n",
       " 0.10833816230297089,\n",
       " 0.1149338111281395,\n",
       " 0.11197840422391891,\n",
       " 0.10927125066518784,\n",
       " 0.11940173804759979,\n",
       " 0.09933899343013763,\n",
       " 0.1092558428645134,\n",
       " 0.11282089352607727,\n",
       " 0.12511500716209412,\n",
       " 0.11775390803813934,\n",
       " 0.10722428560256958,\n",
       " 0.11535479873418808,\n",
       " 0.10368736833333969,\n",
       " 0.10888078808784485,\n",
       " 0.1074264869093895,\n",
       " 0.11344026774168015,\n",
       " 0.12048723548650742,\n",
       " 0.11311756074428558,\n",
       " 0.09290169179439545,\n",
       " 0.1067650243639946,\n",
       " 0.1059105396270752,\n",
       " 0.10007014125585556,\n",
       " 0.1059081107378006,\n",
       " 0.11269502341747284,\n",
       " 0.11301615089178085,\n",
       " 0.10377763211727142,\n",
       " 0.11064132302999496,\n",
       " 0.10022380948066711,\n",
       " 0.12050553411245346,\n",
       " 0.10369168967008591,\n",
       " 0.11454423516988754,\n",
       " 0.10945723950862885,\n",
       " 0.10226869583129883,\n",
       " 0.09721066802740097,\n",
       " 0.10693548619747162,\n",
       " 0.10055261105298996,\n",
       " 0.10932924598455429,\n",
       " 0.10629016160964966,\n",
       " 0.10758312046527863,\n",
       " 0.10732146352529526,\n",
       " 0.11060917377471924,\n",
       " 0.0995631217956543,\n",
       " 0.10423099994659424,\n",
       " 0.11414474248886108,\n",
       " 0.10310085862874985,\n",
       " 0.10980427265167236,\n",
       " 0.1148625835776329,\n",
       " 0.09855067729949951,\n",
       " 0.10297708958387375,\n",
       " 0.11211039125919342,\n",
       " 0.10470076650381088,\n",
       " 0.11319538205862045,\n",
       " 0.11238600313663483,\n",
       " 0.10936041176319122,\n",
       " 0.09937594830989838,\n",
       " 0.1074092835187912,\n",
       " 0.10013701021671295,\n",
       " 0.10811972618103027,\n",
       " 0.11318030208349228,\n",
       " 0.10664153099060059,\n",
       " 0.10486350953578949,\n",
       " 0.11096690595149994,\n",
       " 0.10825123637914658,\n",
       " 0.10443703830242157,\n",
       " 0.10895298421382904,\n",
       " 0.11333440244197845,\n",
       " 0.1180078536272049,\n",
       " 0.11012490093708038,\n",
       " 0.11114825308322906,\n",
       " 0.10768812894821167,\n",
       " 0.10213674604892731,\n",
       " 0.10383391380310059,\n",
       " 0.10962039232254028,\n",
       " 0.1038941964507103,\n",
       " 0.11379220336675644,\n",
       " 0.10764849185943604,\n",
       " 0.112579844892025,\n",
       " 0.10780532658100128,\n",
       " 0.11175630986690521,\n",
       " 0.11192576587200165,\n",
       " 0.11073578894138336,\n",
       " 0.12139417976140976,\n",
       " 0.11794430017471313,\n",
       " 0.12017107009887695,\n",
       " 0.10155797004699707,\n",
       " 0.10433684289455414,\n",
       " 0.10079171508550644,\n",
       " 0.10942941904067993,\n",
       " 0.11298227310180664,\n",
       " 0.11281925439834595,\n",
       " 0.10315001010894775,\n",
       " 0.10773056745529175,\n",
       " 0.11194716393947601,\n",
       " 0.11813780665397644,\n",
       " 0.11472266912460327,\n",
       " 0.10081195086240768,\n",
       " 0.11508713662624359,\n",
       " 0.10503683984279633,\n",
       " 0.1023370772600174,\n",
       " 0.11590462923049927,\n",
       " 0.10461797565221786,\n",
       " 0.11370979249477386,\n",
       " 0.11163868755102158,\n",
       " 0.10918249934911728,\n",
       " 0.09674042463302612,\n",
       " 0.10991039127111435,\n",
       " 0.10730475187301636,\n",
       " 0.1125049740076065,\n",
       " 0.10755960643291473,\n",
       " 0.1024148091673851,\n",
       " 0.11797930300235748,\n",
       " 0.11217159032821655,\n",
       " 0.11265383660793304,\n",
       " 0.10880772769451141,\n",
       " 0.1099092960357666,\n",
       " 0.10735748708248138,\n",
       " 0.09483103454113007,\n",
       " 0.11510980129241943,\n",
       " 0.10103704780340195,\n",
       " 0.11396391689777374,\n",
       " 0.105370432138443,\n",
       " 0.11264495551586151,\n",
       " 0.11270861327648163,\n",
       " 0.10460126399993896,\n",
       " 0.10794568061828613,\n",
       " 0.09654411673545837,\n",
       " 0.1046474426984787,\n",
       " 0.10008813440799713,\n",
       " 0.10849811136722565,\n",
       " 0.10023119300603867,\n",
       " 0.11405813694000244,\n",
       " 0.10516130924224854,\n",
       " 0.1057317852973938,\n",
       " 0.1054646223783493,\n",
       " 0.08604007959365845,\n",
       " 0.10632932186126709,\n",
       " 0.10287892818450928,\n",
       " 0.1089525818824768,\n",
       " 0.09961014986038208,\n",
       " 0.11689183115959167,\n",
       " 0.10205154865980148,\n",
       " 0.1006535217165947,\n",
       " 0.10216115415096283,\n",
       " 0.09211388230323792,\n",
       " 0.09722137451171875,\n",
       " 0.11194058507680893,\n",
       " 0.10627897828817368,\n",
       " 0.10492394864559174,\n",
       " 0.11119014024734497,\n",
       " 0.09997349977493286,\n",
       " 0.11276283115148544,\n",
       " 0.11284353584051132,\n",
       " 0.1130414754152298,\n",
       " 0.10136207193136215,\n",
       " 0.09285011887550354,\n",
       " 0.10429836809635162,\n",
       " 0.10309824347496033,\n",
       " 0.10052459686994553,\n",
       " 0.10485522449016571,\n",
       " 0.11144071072340012,\n",
       " 0.10347890853881836,\n",
       " 0.11482405662536621,\n",
       " 0.09487529844045639,\n",
       " 0.10301196575164795,\n",
       " 0.10850168764591217,\n",
       " 0.10517537593841553,\n",
       " 0.10997737944126129,\n",
       " 0.10731618106365204,\n",
       " 0.11439107358455658,\n",
       " 0.10646952688694,\n",
       " 0.10879623889923096,\n",
       " 0.10551181435585022,\n",
       " 0.09355630725622177,\n",
       " 0.10071619600057602,\n",
       " 0.0942041426897049,\n",
       " 0.09949254989624023,\n",
       " 0.10433318465948105,\n",
       " 0.09827177226543427,\n",
       " 0.11022971570491791,\n",
       " 0.1048201322555542,\n",
       " 0.11179912090301514,\n",
       " 0.09734757244586945,\n",
       " 0.10273300111293793,\n",
       " 0.10883580893278122,\n",
       " 0.10871359705924988,\n",
       " 0.10459717363119125,\n",
       " 0.10782336443662643,\n",
       " 0.09996107220649719,\n",
       " 0.10459717363119125,\n",
       " 0.10161366313695908,\n",
       " 0.10581376403570175,\n",
       " 0.11236634850502014,\n",
       " 0.10814414918422699,\n",
       " 0.10264202207326889,\n",
       " 0.11542990058660507,\n",
       " 0.11483550071716309,\n",
       " 0.10067557543516159,\n",
       " 0.10569415986537933,\n",
       " 0.10582855343818665,\n",
       " 0.10119392722845078,\n",
       " 0.1030084416270256,\n",
       " 0.1112670749425888,\n",
       " 0.11179707944393158,\n",
       " 0.10369573533535004,\n",
       " 0.11585187911987305,\n",
       " 0.10016728937625885,\n",
       " 0.10314678400754929,\n",
       " 0.09687760472297668,\n",
       " 0.11855475604534149,\n",
       " 0.10093244165182114,\n",
       " 0.10736185312271118,\n",
       " 0.10279904305934906,\n",
       " 0.11521326750516891,\n",
       " 0.10372406244277954,\n",
       " 0.11129602789878845,\n",
       " 0.11006556451320648,\n",
       " 0.10107966512441635,\n",
       " 0.11770111322402954,\n",
       " 0.09939216077327728,\n",
       " 0.1067981943488121,\n",
       " 0.11794394254684448,\n",
       " 0.10152262449264526,\n",
       " 0.09335730969905853,\n",
       " 0.1045272946357727,\n",
       " 0.08926966041326523,\n",
       " 0.11123649775981903,\n",
       " 0.10149991512298584,\n",
       " 0.12207775563001633,\n",
       " 0.09451719373464584,\n",
       " 0.11361457407474518,\n",
       " 0.10851866006851196,\n",
       " 0.10631297528743744,\n",
       " 0.10559795796871185,\n",
       " 0.10239651054143906,\n",
       " 0.1103232353925705,\n",
       " 0.10499964654445648,\n",
       " 0.11491654813289642,\n",
       " 0.09945856034755707,\n",
       " 0.10274031013250351,\n",
       " 0.09211210906505585,\n",
       " 0.10938068479299545,\n",
       " 0.10686157643795013,\n",
       " 0.10544693470001221,\n",
       " 0.10425441712141037,\n",
       " 0.10448235273361206,\n",
       " 0.1011425256729126,\n",
       " 0.09968322515487671,\n",
       " 0.11045020818710327,\n",
       " 0.09839245676994324,\n",
       " 0.09833984076976776,\n",
       " 0.09915386140346527,\n",
       " 0.10875731706619263,\n",
       " 0.0983802080154419,\n",
       " 0.10290734469890594,\n",
       " 0.10561241209506989,\n",
       " 0.10896919667720795,\n",
       " 0.08947703242301941,\n",
       " 0.09304794669151306,\n",
       " 0.10307393968105316,\n",
       " 0.10198429226875305,\n",
       " 0.10456739366054535,\n",
       " 0.10986679792404175,\n",
       " 0.10470068454742432,\n",
       " 0.09795757383108139,\n",
       " 0.11035431921482086,\n",
       " 0.0936715230345726,\n",
       " 0.1093125194311142,\n",
       " 0.10593269765377045,\n",
       " 0.11570876836776733,\n",
       " 0.10693248361349106,\n",
       " 0.10889556258916855,\n",
       " 0.10947221517562866,\n",
       " 0.10976201295852661,\n",
       " 0.10435466468334198,\n",
       " 0.10983477532863617,\n",
       " 0.09904409945011139,\n",
       " 0.1089879497885704,\n",
       " 0.10101217031478882,\n",
       " 0.10571914911270142,\n",
       " 0.11364730447530746,\n",
       " 0.10042493045330048,\n",
       " 0.1070980355143547,\n",
       " 0.09494226425886154,\n",
       " 0.11929339170455933,\n",
       " 0.10972899198532104,\n",
       " 0.10882335901260376,\n",
       " 0.1008325070142746,\n",
       " 0.10005458444356918,\n",
       " 0.09301513433456421,\n",
       " 0.10298600047826767,\n",
       " 0.11098265647888184,\n",
       " 0.11299657076597214,\n",
       " 0.10599786043167114,\n",
       " 0.1036703810095787,\n",
       " 0.10371963679790497,\n",
       " 0.0938347801566124,\n",
       " 0.10661777853965759,\n",
       " 0.1133827194571495,\n",
       " 0.1098850667476654,\n",
       " 0.09780269116163254,\n",
       " 0.09671297669410706,\n",
       " 0.09366687387228012,\n",
       " 0.10577445477247238,\n",
       " 0.10306672751903534,\n",
       " 0.11355791985988617,\n",
       " 0.100028857588768,\n",
       " 0.09710437059402466,\n",
       " 0.10632829368114471,\n",
       " 0.1068200096487999,\n",
       " 0.10463708639144897,\n",
       " 0.10528149455785751,\n",
       " 0.11174823343753815,\n",
       " 0.10581623017787933,\n",
       " 0.10187589377164841,\n",
       " 0.10239838808774948,\n",
       " 0.10511128604412079,\n",
       " 0.09672406315803528,\n",
       " 0.09957316517829895,\n",
       " 0.10043953359127045,\n",
       " 0.10468365252017975,\n",
       " 0.09657137840986252,\n",
       " 0.09515345096588135,\n",
       " 0.1083139032125473,\n",
       " 0.10999488830566406,\n",
       " 0.10286961495876312,\n",
       " 0.1025736853480339,\n",
       " 0.11095403879880905,\n",
       " 0.0994332879781723,\n",
       " 0.09892803430557251,\n",
       " 0.10196294635534286,\n",
       " 0.10811879485845566,\n",
       " 0.0983915776014328,\n",
       " 0.10131371021270752,\n",
       " 0.10022856295108795,\n",
       " 0.09558294713497162,\n",
       " 0.09884770214557648,\n",
       " 0.09870769083499908,\n",
       " 0.0927068218588829,\n",
       " 0.09197624772787094,\n",
       " 0.11466475576162338,\n",
       " 0.10871215909719467,\n",
       " 0.1065487265586853,\n",
       " 0.1024094745516777,\n",
       " 0.10424655675888062,\n",
       " 0.10400800406932831,\n",
       " 0.10741560906171799,\n",
       " 0.10459202527999878,\n",
       " 0.11918904632329941,\n",
       " 0.11231140792369843,\n",
       " 0.09291302412748337,\n",
       " 0.10416882485151291,\n",
       " 0.10517343133687973,\n",
       " 0.11459419131278992,\n",
       " 0.10335531085729599,\n",
       " 0.08945624530315399,\n",
       " 0.09986662864685059,\n",
       " 0.10600745677947998,\n",
       " 0.097959004342556,\n",
       " 0.10104304552078247,\n",
       " 0.10123489797115326,\n",
       " 0.10055533051490784,\n",
       " 0.09265086054801941,\n",
       " 0.10592692345380783,\n",
       " 0.099273681640625,\n",
       " 0.10157909244298935,\n",
       " 0.10552897304296494,\n",
       " 0.10098840296268463,\n",
       " 0.10470615327358246,\n",
       " 0.10700052976608276,\n",
       " 0.10906411707401276,\n",
       " 0.10614906251430511,\n",
       " 0.10583716630935669,\n",
       " 0.10602002590894699,\n",
       " 0.10110098123550415,\n",
       " 0.10044650733470917,\n",
       " 0.08862185478210449,\n",
       " 0.1053662896156311,\n",
       " 0.10521882772445679,\n",
       " 0.09786754101514816,\n",
       " 0.09703312814235687,\n",
       " 0.09928470104932785,\n",
       " 0.11213447153568268,\n",
       " 0.10302971303462982,\n",
       " 0.10403181612491608,\n",
       " 0.09757882356643677,\n",
       " 0.10665082931518555,\n",
       " 0.10324607789516449,\n",
       " 0.10076811164617538,\n",
       " 0.08908042311668396,\n",
       " 0.10861881077289581,\n",
       " 0.10498183220624924,\n",
       " 0.10392048954963684,\n",
       " 0.09591369330883026,\n",
       " 0.10151361674070358,\n",
       " 0.09831158816814423,\n",
       " 0.09825004637241364,\n",
       " 0.08908408135175705,\n",
       " 0.10107825696468353,\n",
       " 0.10842897742986679,\n",
       " 0.08907097578048706,\n",
       " 0.09963832795619965,\n",
       " 0.0952799916267395,\n",
       " 0.09848203510046005,\n",
       " 0.10715972632169724,\n",
       " 0.10073646903038025,\n",
       " 0.09745866060256958,\n",
       " 0.10287433862686157,\n",
       " 0.10824074596166611,\n",
       " 0.09519865363836288,\n",
       " 0.11192810535430908,\n",
       " 0.09855340421199799,\n",
       " 0.10561984777450562,\n",
       " 0.10192009806632996,\n",
       " 0.1014077365398407,\n",
       " 0.10429012030363083,\n",
       " 0.09112634509801865,\n",
       " 0.10102277994155884,\n",
       " 0.09140729904174805,\n",
       " 0.10099242627620697,\n",
       " 0.09571707248687744,\n",
       " 0.10324807465076447,\n",
       " 0.10589884966611862,\n",
       " 0.11343945562839508,\n",
       " 0.10570316016674042,\n",
       " 0.09320437908172607,\n",
       " 0.10679666697978973,\n",
       " 0.10339643806219101,\n",
       " 0.10450595617294312,\n",
       " 0.1127709299325943,\n",
       " 0.11025252193212509,\n",
       " 0.09783285111188889,\n",
       " 0.096826933324337,\n",
       " 0.10327553749084473,\n",
       " 0.10625007003545761,\n",
       " 0.10299770534038544,\n",
       " 0.09278303384780884,\n",
       " 0.09353961050510406,\n",
       " 0.09531047195196152,\n",
       " 0.08800791949033737,\n",
       " 0.10288459062576294,\n",
       " 0.10492090880870819,\n",
       " 0.10865484178066254,\n",
       " 0.09998147934675217,\n",
       " 0.09966209530830383,\n",
       " 0.09597035497426987,\n",
       " 0.09912313520908356,\n",
       " 0.09363055229187012,\n",
       " 0.09623213857412338,\n",
       " 0.09746571630239487,\n",
       " 0.08910340070724487,\n",
       " 0.106443390250206,\n",
       " 0.10235485434532166,\n",
       " 0.1131654679775238,\n",
       " 0.09519290924072266,\n",
       " 0.108746737241745,\n",
       " 0.09878747165203094,\n",
       " 0.10020049661397934,\n",
       " 0.09841104596853256,\n",
       " 0.1087082028388977,\n",
       " 0.09174331277608871,\n",
       " 0.11042942851781845,\n",
       " 0.10343202203512192,\n",
       " 0.10206367820501328,\n",
       " 0.09127321094274521,\n",
       " 0.09813998639583588,\n",
       " 0.1065300926566124,\n",
       " 0.10338222980499268,\n",
       " 0.09484182298183441,\n",
       " 0.09333883225917816,\n",
       " 0.10483967512845993,\n",
       " 0.10053394734859467,\n",
       " 0.10143981873989105,\n",
       " 0.10695561021566391,\n",
       " 0.09250454604625702,\n",
       " 0.095772385597229,\n",
       " 0.09911035001277924,\n",
       " 0.0908394381403923,\n",
       " 0.10071682184934616,\n",
       " 0.1014690026640892,\n",
       " 0.10802274942398071,\n",
       " 0.10229428857564926,\n",
       " 0.10782017558813095,\n",
       " 0.10051858425140381,\n",
       " 0.10542504489421844,\n",
       " 0.09907176345586777,\n",
       " 0.1055479496717453,\n",
       " 0.10365905612707138,\n",
       " 0.1003185361623764,\n",
       " 0.10469545423984528,\n",
       " 0.10315641015768051,\n",
       " 0.10517878830432892,\n",
       " 0.0932358130812645,\n",
       " 0.10729329288005829,\n",
       " 0.09130939096212387,\n",
       " 0.09811922907829285,\n",
       " ...]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# model = NerfModel(N=256).to(g_device)\n",
    "\n",
    "model = NerfModel(N=256)\n",
    "# prev_mode_path = \"/Users/hyunjae.k/110_HyunJae_Git/2025_Drills/DL_Drills/NERF/temp/05_PlenoxelsNeRF_models/nerf_model_006.pth\"\n",
    "# model.load_state_dict(torch.load(prev_mode_path))\n",
    "model.to(g_device)\n",
    "\n",
    "\n",
    "model_optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(model_optimizer, milestones=[2, 4, 8], gamma=0.5)\n",
    "\n",
    "data_loader = DataLoader(training_dataset, batch_size=1024, shuffle=True)\n",
    "# data_loader = DataLoader(training_dataset, batch_size=2048, shuffle=True)\n",
    "train(model, model_optimizer, scheduler, data_loader, nb_epochs=16, device=g_device, hn=2, hf=6, nb_bins=192)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a6935af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import Video\n",
    "\n",
    "# Video(\"./temp/02_kiloNeRF_novel_views/nerf_model_003.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6fb1b62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test(model, 2, 6, testing_dataset, chunk_size = 10000, img_index=1, nb_bins=192)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15999b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = NerfModel(N=256)\n",
    "# prev_mode_path = \"/Users/hyunjae.k/110_HyunJae_Git/2025_Drills/DL_Drills/NERF/temp/05_PlenoxelsNeRF_models/nerf_model_015.pth\"\n",
    "# model.load_state_dict(torch.load(prev_mode_path))\n",
    "# model.to(g_device)\n",
    "\n",
    "# model.eval()\n",
    "# imgT_lst = []\n",
    "# for idx in tqdm(range(200), desc=\"Validation\"):\n",
    "#     imgT_lst.append(test(model, 2, 6, testing_dataset, chunk_size = 10, img_index=idx, nb_bins=192).unsqueeze(0))\n",
    "# img_t = (torch.cat(imgT_lst) * 255).to(torch.uint8)\n",
    "# _video_path = os.path.join(g_video_path,  f'nerf_model_test.mp4')\n",
    "# torchvision.io.write_video(_video_path, img_t, fps)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gsplat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
